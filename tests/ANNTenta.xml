<?xml version="1.0" encoding="utf-8" ?>
<Test Name="Tenta">
	<TestQuestion Category="Grundläggande">
		<Question>
			Ange vad olika delar av ett artificiellt neuron nätverk tar inspiartion ifrån biologin.
		</Question>
		<Answer>
			[ul]
				[li]Inlinje till en nod: dendrite.[/li]
				[li]Utlinje för en nod: axon.[/li]
				[li]Utvärdet för en nod: utfrekvensen för en neuron.[/li]
				[li]Vikterna mellan två noder: synaps effektiviteten.[/li]
				[li]Summerings komponenten hos en nod: soma.[/li]
				[li]Överföringsfunktionen för en nod: soma spatial och temporal summerings egenskaper.[/li]
				[li]Vikterna kan vara positiva eller negativa: Excitatory and inhibitory synaptisk 
     potentiell.[/li]
     			[li]Action potential av en neuron: Puls som skickas via en axon av en neuron.[/li]
				[li]Trösklad överföringsfunktion: allt-eller-inget av action potentials.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad säger "no free lunch theorem"?
		</Question>
		<Answer>
			Att ta genomsnittet över alla världar (datamängder/problem) ger att det inte finns någon optimal algoritm. Men, givet en specifik domän så finns det algoritmer som är bättre än andra.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad är en aktiveringsfunktion?
		</Question>
		<Answer>
			Den anger när en nod "aktiveras". Olika typer är linjär, tröskel och sigmoid.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Ange olika typer av inlärningsregler.
		</Question>
		<Answer>
			[ul]
				[li]Supervised: En "lärare" ger det korrekta svaret.[/li]
				[li]Unsupervised: Endast indata är tillgängligt.[/li]
				[li]Reinforcement: Belöning fås som indikator på hur "rätt" svaret är.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad innebär generalisering?
		</Question>
		<Answer>
			Att ge rätt svar för indata som inte fanns med under träning.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Ange olika sätt att mäta prestandan.
		</Question>
		<Answer>
			[ul]
				[li]Felfrekvensen för träningsdata.[/li]
				[li]Felfrekvensen för ny data.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad innebär överinlärning?
		</Question>
		<Answer>
			Det innebär att nätverket lär sig träningsdatan, i stället för den underliggande funktionen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad kan man göra för att undvika överinlärning?
		</Question>
		<Answer>
			[ul]
				[li]Använda sig av färre gömda noder (om nätverket har flera lager).[/li]
				[li]Stoppa träningen i förtid.[/li]
				[li]Använd mer indata.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
		<TestQuestion Category="Grundläggande">
		<Question>
			Vad kan man göra för att förbättra generaliseringen?
		</Question>
		<Answer>
			[ul]
				[li]Lägga till brus till träningsdatan. Syftet är att sprida ut datapunkterna för att för förhindra en precis anpassning till original datan.[/li]
				[li]Early stopping.[/li]
				[li]Pruning.[/li]
				[li]Bootstrapping.[/li]
				[li]n-fold cross validation.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad kan man göra för att reducera (pruning) en modells komplexitet?
		</Question>
		<Answer>
			[ul]
				[li]Ta bort onödiga gömda noder.[/li]
				[li]Ta bort svaga vikter.[/li]
			[/ul]Detta kan uppnås genom att inkludera en bestraffning i kostnadsfunktionen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad innebär bias-varians-dilemmat?
		</Question>
		<Answer>
			Bias är felet som uppstår på grund av felaktiga antagande i inlärningsalgoritmen. Hög bias kan leda till att algoritmen missar relevanta relationer mellan särdrag och faktisk utdata (underinlärning).

			Variansen är felet som uppstår på grund av känslighet för små variationer i träningsdatan. Hög varians kan leda till överinlärning. 

			Bias-varians-dilemmat innebär att man antigen får underinlärning eller överinlärning.

			Det beskrivs av följande bild:
			[img]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/BiasVariance.png[/img].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Hur fungerar early stopping?
		</Question>
		<Answer>
			Det kräver att man har en extra datamängd, valideringsmängden (som är en del av originaldatan). Nätverket tränas då till dess att felet som uppmäts i valideringsmängden når ett minimum. I praktiken finns det oftast flera lokala minimum, så då använder man någon form av heuristik för att bestämma när träningen ska avslutas. Ett exempel är att ett givet antal lokala minimums har påträffats i följd.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad innebär regularisation?
		</Question>
		<Answer>
			Det är ett sätt att kontrollera komplexiteten hos en modell. Detta fungerar genom att man introducerar en bestraffing i felfunktionen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Hur kan man implementera regularisation?
		</Question>
		<Answer>
			[b]Weight decay[/b]
			Det innebär att man bestraffar stora vikter, genom att tvinga att tvinga stora vikter till 0.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Ange de antaganden som olika typer av algoritmer gör.
		</Question>
		<Answer>
			[ul]
				[li][b]Boltzmann-maskin:[/b] Inga antaganden.[/li]
				[li][b]Hopfield:[/b] Att indata är okorrelerade/oberoende.[/li]
				[li][b]Backprop:[/b] Beslutsytor enligt viktat medelvärde.[/li]
				[li][b]SOM, PCA:[/b] Dimensioner med hög varians är viktiga.[/li]
				[li][b]ICA:[/b] Bruset är normalfördelat.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grundläggande">
		<Question>
			Vad innebär "cross validation"?
		</Question>
		<Answer>
			Det innebär att man delar in data i olika delar. Nätverket tränas på en viss del av datan (training) och evalueras under träning på en annan del (testing). Det finns också en tredje del, validation som används för att evaluera prestanden (t.ex. mellan olika typer av algoritmer).
		</Answer>
	</TestQuestion>
		<TestQuestion Category="Grundläggande">
		<Question>
			Hur fungerar "n-fold cross validation"?
		</Question>
		<Answer>
			Data delas in i delar för träning och validering där var [math]n[/math]:te värde används som validering. Indelningen av mätvärdena görs slumpmässigt i [math]n[/math] ungefär lika stora delar. Detta ger att [math]n-1[/math] delar används för träning och en del för validering. Detta upprepas sedan [math]n[/math] gånger och medelvärdet av de [math]n[/math] prediktionsfelen används som en uppskattning av felet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad händer om vi kopplar ihop flera linjära nätverk?
		</Question>
		<Answer>
			Det resulterar fortfarande i ett linjärt nätverk. Det för att om: Låt [math]W_1, W_2, W_3[/math] representera tre viktmatriser för tre linjära nätverk. Det sammankopplade nätverket ges av: [math]W_3(W_2(W_1(x)))=(W_3 * W_2 * W_1) * x[/math] där [math]x[/math] är indata.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad innebär Hebbs inlärningshypotes?
		</Question>
		<Answer>
			Att simultana aktiveringar mellan två neuroner stärker synaptiska anslutningen mellan dem. Matematisk beskrivs det ofta som: [math]\Delta w = x_j * y_i[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad för problem finns det med Hebbs inlärningsregel?
		</Question>
		<Answer>
			Att den tillåter att vikter växer utan gränser. Detta problem kan lösas genom att subtraherar vikterna med [math]y*w[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad är en "Threshold Logic Unit"?
		</Question>
		<Answer>
			Det innebär att en neuron aktiveras (d.v.s. ger 1 i utvärde) endast om: [math]\sum_i w_i x_i > \theta[/math], annars ges värdet 0 där [math]\theta[/math] är tröskelvärdet. Det går även att modifiera ekvationen så att den i stället skrivs som: [math]w_0 * 1 + \sum_i w_i x_i > 0[/math] där [math]w_0 = -\theta[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad innebär det att något är linjärt separerbart?
		</Question>
		<Answer>
			Det innebär att datan kan delas in av en linje (2D) eller ett plan (högre dimensioner) där ena sidan anger första klassen och den andra sidan den andra klassen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Hur kan ett trösklat nätverk tränas (även kallad perceptionsinlärning regeln)?
		</Question>
		<Answer>
			Genom att ändra vikter när data klassificeras fel.
			Om resultatet är 0, men borde vara 1, ändras vikterna: [math]\Delta w = \eta x[/math].
			Om resultatet är 1, men borde vara 0, ändras vikterna: [math]\Delta w = -\eta x[/math].

			Alternativt: Låt [math]e=t-y[/math] vara felet där [math]t[/math] är det verkliga värdet och [math]y[/math] är resultatet (som är trösklat). Då uppdateras vikterna enligt: [math]\Delta w = \eta e x[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad säger konvergenssatsen för perceptionsinlärning?
		</Question>
		<Answer>
			Om det finns en lösning (dvs är linjärt separerbart) för en ändlig träningsdatamängd, så kommer nätverket alltid att konvergera efter ett ändligt antal steg, oberoende av steglängden [math]\eta[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Vad för problem finns det med perceptionsinlärning?
		</Question>
		<Answer>
			Att inlärningens stoppas onödigt tidigt. Detta på grund av att inlärningen stoppas när alla träningsmönster har klassificeras korrekt, men detta betyder inte att alla beslutsytor har placerats optimalt givet den informationen som finns tillgängligt via träningsdatan. Detta ger att mönster som ej sågs under inlärning kan klassificeras felaktigt.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="SingleLayer">
		<Question>
			Hur fungerar deltaregeln?
		</Question>
		<Answer>
			Låt [math]e = t-w^Tx[/math] vara felet (d.v.s. skillnaden mellan det verkliga värdet och det värde som skickas ut). Sen ska då vikterna hittas som minimerar funktionen: [math]\xi = e^2/2[/math]. En enkel algoritm är då att använda sig av "gradient decent", där gradienten är riktningen där felet ökar som mest (och vi tar då motsatt riktning).

			Detta ger att vikterna ska ändras enligt: [math]\Delta w = \eta e x[/math],
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Kan ett flerlagersnätverk alltid extrahera ett komplext område?
		</Question>
		<Answer>
			Ja, givet att det finns tillräckligt med gömda noder.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Vad säger universal approximations satsen?
		</Question>
		<Answer>
			Att ett framåtkopplat nätverk med ett lager av ett ändligt antal gömda noder kan approximera en godtycklig kontinuerlig funktion på en kompakt delmängd av [math]R^n[/math] där aktiveringsfunktionen är begränsad, monotont ökande och kontinuerlig. Notera att satsen säger inget om vad för algoritm som krävs för att få parametrarna till nätverket.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Varför fungerar inte inlärningsreglerna för enlagersnätverk också för flerlagersnätverk?
		</Question>
		<Answer>
			[b]Perceptionsinlärning[/b]
			Kräver att vi vet vilken riktning vikterna ska ändras för att komma nära lösningen.

			[b]Deltaregeln[/b]
			Kräver att vi kan mäta felet innan vi trösklar. Fungerar endast för det sista lagret.

			Lösningen är att använda en funktion som ser ut som att vi trösklar, men som är deriverbar.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Ange olika funktioner som används som tröskelfunktioner i flerlagersnätverk.
		</Question>
		<Answer>
			Sigmoid (?): [math]\phi(x)=(1-e^-x)/(1+e^-x)[/math].
			Arctan: [math]\phi(x)=arctan(x)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Hur fungerar backprop?
		</Question>
		<Answer>
			Den består av tre delar:
			[b]Framåtpassen[/b]
			Beräknar aktiveringen av alla noder, i alla lager.
			Beräkna alla: [math]h_j = \phi(\sum_i v_(j, i) x_i)[/math] och [math]y_k = \phi(\sum_i w_(k, j) h_j)[/math] där [math]h_j[/math] är en gömd nod, [math]v_(j, i)[/math] en vikt i det första lagret (gömda), [math]y_k[/math] är en utnod och [math]w_(k, j)[/math] en vikt i det andra lagret (output).

			[b]Bakåtpassen[/b]
			Distribuerar de generaliserade felet mellan alla noder.
			Beräkna alla: [math]\delta_k = (t_k - y_k)*\phi'(y_k^text{in})[/math] och [math]\delta_j = \sum_k \delta_k * w_(k, j) * \phi'(h_j^{text}))[/math].

			[b]Uppdatera vikter[/b]
			[math]\delta w_(k, j) = \eta \delta_k h_j[/math] och [math]\delta v_(j, i) = \eta \delta_j x_i[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Vad för problem finns det med backprop?
		</Question>
		<Answer>
			[ul]
				[li]Konvergerar inte alltid (fastnar i lokalt minimum).[/li]
				[li]Konvergerar långsamt.[/li]
				[li]Många parameterar måste finjusteras.[/li]
				[li]Skalar dåligt för stora problem.[/li]
				[li]Biologiskt orealistisk.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Varför kan backprop misslyckas att hitta en bra lösningen, även om det finns?
		</Question>
		<Answer>
			Backprop använder sig av gradinent decent. Denna algoritm kan fastna i lokala minimum, som den inte kan undkomma ifrån och lokala minimum behöver inte representera en bra lösning.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Vad bör man tänka på när man använder sig av backprop?
		</Question>
		<Answer>
			[ul]
				[li]Använd en asymmetrisk [math]\phi(x)[/math]-funktion.[/li]
				[li]Lägg målvärdena [math]t[/math] inom samma domän som [math]\phi[/math]. Syftet är att öka inlärningshastigheten genom att undvika små derivator i "svansen" på överföringsfunktionen.[/li]
				[li]Ordna eller vikta träningsexemplen så att svåra exempel dominerar. Syftet är att nätverket ska tränas mer på de svåra exemplerna, så att beslutsytorna anpassas efter dessa.[/li]
				[li]Välj smarta initialvikter.[/li]
				[li]Introducera momentum i viktuppdateringen. Syftet är att undvika att om enskilda mönster innehåller mycket brus, skulle det ge stora variationer i vikterna.[/li]
				[li]Lägg till slumpmässigt brus till vikterna under träning. Syftet är att undvika att fastna i lokala minimum.[/li]
				[li]Ta bort [math]\phi(x)[/math]-funktionen för utnoder.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Vad kan hända om antalet gömda noder ökas i ett flerlagersnätverk?
		</Question>
		<Answer>
			[ul]
				[li]Att mer antal träningsdata behövs, för att de blir fler vikter.[/li]
				[li]Det blir möjligt att approximera mer komplexa funktioner, för att flera gömda enheter ger flera beslutsytor.[/li]
				[li]Det finns risken för att det leder till sämre generalisering (överinlärning), när man har för många. Har man dock för få noder, så förbättras generalisering.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Vad är en "Support Vector Machine"?
		</Question>
		<Answer>
			Det är något som transformerar indata till högdimensionella rum och väljer den separering som har maximala marginaler.

			[b]Fördelar[/b]
			[ul]
				[li]Mycket bra på att generalisera.[/li]
				[li]Fungerar bra även med lite träningsdata.[/li]
				[li]Snabb klassificering.[/li]
			[/ul][b]Nackdelar[/b]
			[ul]
				[li]Icke-lokala viktberäkningar.[/li]
				[li]Svårt att implementera effektivt.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MultiLayer">
		<Question>
			Vad är "vanishing gradient" för problem?
		</Question>
		<Answer>
			Det är ett problem som uppstår i nätverk som använder sig av gradienten för att uppdatera vikterna (t.ex. backprop). Problemet är att gradienten (felet som vikterna ska uppdateras med i varje lager) sjunker exponentiellt för varje lager (sett från utlagret). Detta gör det oftast opraktisk att ha många lager som använder sig av Backprop.

			Ett relaterat problem är [i]exploding gradient[/i], där gradienten istället får för stora värden.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad innebär "Vector Quantization"?
		</Question>
		<Answer>
			Det delar in en mängd punkter (~vektorer) i grupper, där varje grupp har ungefär lika många punkter närmast gruppen. Varje grupp representeras av ett center.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad innebär competitive learning?
		</Question>
		<Answer>
			Det innebär att man endast uppdaterar noden som "vinner".
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad är döda noder för något, och vad kan man göra för att undvika att dessa uppstår?
		</Question>
		<Answer>
			Det innebär att noder som ligger långt från datan aldrig kommer att vinna, och därför ej förbättras. För att undvika att det uppstår kan man t.ex. göra:
			[ul]
				[li]Initialisera noderna m.h.a. riktig data.[/li]
				[li]Starta från en punkt.[/li]
				[li]Man tillåter att alla noder alltid lär sig lite.[/li]
				[li]Man låter "förlorande" noder vinna lättare.[/li]
				[li]Grannar till vinnaren uppdateras också.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad är en "Radial-basis function" (RBF) för något?
		</Question>
		<Answer>
			Det är en funktion, där funktionens värde endast beror på avståndet från ett center (t.ex. origo). Det vill säga: Låt [math]\phi(vec x)[/math] vara en RBF-funktion och [math]vec x[/math] en vektor. Då gäller: [math]\phi(vec x) = \phi(||vec x||)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Hur fungerar ett RBF-nätverk (Radial-basis function)?
		</Question>
		<Answer>
			Det består av tre lager: inlagret, det gömda lagret och utlagret. Varje nod i det gömda lagret har en överföringsfunktion av typen [i]radial-basis[/i] där varje nod har en position (dess center). Utlagret beräknar sedan utdatan som summan av de [math]n[/math] gömda noderna: [math]hat f(x)=\sum_i^n w_i \phi_i(x)[/math] där [math]hat f(x)[/math] är en approximation av funktionen [math]f(x)[/math].

			Inlärningen består av två steg:
			Först placeras RBF-noderna, med hjälp av självorganiserade inlärningsregler: t.ex. competitive learning eller expectation maximization. Sedan beräknas vikterna med minstakvadratmetoden eller deltaregeln.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad för fördelar finns det att använda RBF som aktiveringsfunktion istället för sigmoid?
		</Question>
		<Answer>
			RBF är lokala medans sigmoid har oändligt långa svansar. Med lokala menas här att en RBF funktion har en begränsad räckvidd. Detta ger att under inlärning, så påverkas inte en RBF-nod av alla träningsdata, vilket ger att inlärning av ett mönster behöver inte innebär att det påverkar inlärningen av ett annat mönster.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad används oftast ett RBF-nätverk till?
		</Question>
		<Answer>
			För att ändra på representationen av indata.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Hur fungerar Competitive Learning för Vector Quantization?
		</Question>
		<Answer>
			I varje iteration så väljs en slumpmässig träningsvektor ifrån data:n. Den närmaste RBF-enheten (vinnaren) beräknas sedan (t.ex. med euklidiska avståndet), och denna enhet uppdateras sedan på ett sätt så att den kommer närmare träningsvektorn.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Hur fungerar Expectation Maximization?
		</Question>
		<Answer>
			I varje iteration består av att först vikta alla datapunkter av värdet för punkten för varje basfunktion, och sedan flytta varje enhet emot deras respektive centrum, och justerar storleken på enheterna (genom att ändra variansen).
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Vad är en "Self Organizing Map"?
		</Question>
		<Answer>
			Det är en avbildning (map) som bevarar topologin inom datan. D.v.s. om två punkter i indatan är nära varandra, kommer de även vara nära varandra i avbildningen. Den kan också användas för att reducera dimensionen på indata, från högdimensionell till låg (1D, 2D, 3D).

			Den inlärningsprincip som används är "competitive learning" där vinnarens grannar också får ta del på vinsten. Man låter även storleken på grannskapet variera under inlärningsprocessen, där den i början är stor och sedan minskar.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="CompetitiveLearning">
		<Question>
			Hur lyder algoritmen för SOM?
		</Question>
		<Answer>
			[ol]
				[li]Beräkna likheten mellan indata och vikterna som inkommer vid varje utnod.[/li]
				[li]Hitta den nod som är mest likt, [i]vinnaren[/i].[/li]
				[li]Välj ett antal utnoder som ligger nära vinnaren i utnätet. Detta kallas för grannskapet.[/li]
				[li]Uppdatera vikterna för alla noder i grannskapet så att deras vikter flyttas närmare indatan.[/li]
			[/ol]
			[b]Beräkna likheten[/b]
			Det som oftast används är det euklidiska avståndet mellan indata och vikt-vektorn för noden.

			[b]Viktuppdatering[/b]
			Sker enligt: [math]w_i \leftarrow w_i + \eta (x-w_i)[/math] där [math]w_i[/math] är vikt-vektorn för den [math]i[/math]:te utnoden.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DataProcessing">
		<Question>
			Vad kan man göra för att "tvätta" data?
		</Question>
		<Answer>
			[ul]
				[li]Hitta data som saknas.[/li]
				[li]Ta bort data med låg kvalitet.[/li]
				[li]Normalisera data så att den har 0 i medelvärdet och 1 standardavvikelse.[/li]
				[li]Använd lågpass-filtrering för att ta bort brus.[/li]
				[li]Använd högpass-filtrering för att ta bort "slow drifts" (sakta spikar?).[/li]
				[li]Ta bort diskontinuitet.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DataProcessing">
		<Question>
			Vad är "Principal Component Analsysis" (PCA)?
		</Question>
		<Answer>
			Det är en algoritm för att reducera dimensionen på indata. Den antar att informationen ligger i datans spridning (varians). En [i]principalkomponent[/i] är riktningen för de största variationen i data:n. Sedan försöker man passa en [math]N[/math]-dimensionell ellipsoid till datan, som transformerar data till ett nytt koordinatsystem. Detta innebär att PCA är en linjär transformation.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DataProcessing">
		<Question>
			Vad innebär "Ensemble learning"?
		</Question>
		<Answer>
			Det innebär att man använder sig av flera nätverk. I alla nätverk, så kan bias minskas till kostnad av ökad varians. Men i en samling av nätverk, så kan variansen minskas utan att det behöver påverka biasen.	Idén är att en mängd av nätverk med hög bias kan kombineras för att producera ett nytt nätverk med låg bias. 
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DataProcessing">
		<Question>
			Vad innebär "Ensemble averaging"?
		</Question>
		<Answer>
			Det innebär att vi genererar [math]N[/math] stycken experter, där varje expert har egna initialvärden. Varje expert tränas sedan på en egen partition av indatan. Dessa kombineras genom att antingen ta medelvärdet eller genom röstning (ta då resultatet som fått flest röster).
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad är ett "Recurrent Neural Network" (RNN) för något?
		</Question>
		<Answer>
			Det innebär att varje i nod i nätverket är kopplade till varandra, vilket bildar en riktad cyklisk graf.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad är en Boltzmann-maskin för något?
		</Question>
		<Answer>
			Det är en typ av stokastisk RNN. Noderna delas in i gömda och synliga noder, där de synliga noderna är de som får information från miljön. Om en nod [math]i[/math] aktiveras eller inte beror på dess sannolikhet, som följer Boltzmannfördelningen: [math]P_i(on) = 1 / (1+e^(-(\Delta E_i) / T))[/math] där [math]\Delta E_i[/math] är skillnanden i energi (mellan att vara på och av) för nod [math]i[/math] och [math]T[/math] temperaturen för systemet.

			Nätverket tränas genom att vikterna justeras så att interngenererad aktivitet motsvarar extern aktivitet. Nätverket kräver även att inga noder är kopplade till sig själva och att vikterna är symmetriska: [math]w_{i, j} = w_{j, i}[/math] (annars garanteras ej nätverket att konvergera).
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad för typer av villkor (constraints) finns det i Boltzmann-maskiner?
		</Question>
		<Answer>
			[b]Hårda (conditions)[/b]
			Starka vikter.

			[b]Mjuka (wishes)[/b]
			Svaga vikter.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad för problem finns det med Boltzmann-maskinen?
		</Question>
		<Answer>
			Att antalet beräkningar som krävs ökar exponentiellt med storleken av problemet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad kan man göra för att undvika problemen med Boltzmann-maskinen?
		</Question>
		<Answer>
			[ul]
				[li]Reducera topologin: Inga anslutningar mellan gömda noder (begränsade Boltzmann-maskinen).[/li]
				[li]Ersätta stokastiska variabler med deras medelvärde (Hopfieldnätverket)[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Hur beräknas viktmatrisen för ett Hopfieldnätverk?
		</Question>
		<Answer>
			Ett element i viktmatrisen kan skrivas som: [math]w_{i,j} = 1/N \sum_{\mu=1}^P x_i^{\mu} x_j^{\mu}[/math] där [math]x^\mu[/math] är ett mönster som ska lagras.  Man bör även se till att diagonalen i denna matris är noll.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Hur beräknas aktiveringen för ett mönster?
		</Question>
		<Answer>
			Enligt: [math]x_i = sign(\sum_j w_{i, j} x_j)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Ange de två olika ordningarna för uppdateringar.
		</Question>
		<Answer>
			[ul]
				[li]Synkront: Alla noder beräknar sin aktivering, och sedan uppdaterar sina tillstånd simultant.[/li]
				[li]Asynkront: En nod i taget uppdaterar dess aktivering och tillstånd. Ordningen kan antigen vara fix eller slumpmässig.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Hur definieras energin i ett Hopfieldnätverk?
		</Question>
		<Answer>
			[math]E = -1/2 \sum_i \sum_j w_{i, j} x_i x_j[/math]. I matrisform blir det: [math]E=-1/2 * (W * x^T)^T * x^T[/math] där [math]x[/math] är en radvektor. Det är lättare att låta [math]x[/math] vara en kolumnvektor istället, för då blir formeln: [math]E=-1/2 * (W * x)^T * x[/math]. Det är även möjligt att ta bort faktorn [math]1/2[/math] för det är endast en skalfaktor.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad är en attraktor (attractor) i ett Hopfieldnätverk?
		</Question>
		<Answer>
			Det är något som indata kommer att konvergera mot, om uppdateringsregeln används. Varje attraktor i nätverket är ett lokalt minimum (sett till energin). De mönster man försöker lagra vill man ska vara attraktorer.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad är ett "spurious state" i ett Hopfieldnätverk?
		</Question>
		<Answer>
			Det är stabila tillstånd som inte motsvarar något mönster som lärdes uppstår. Dessa uppstår som en sidoeffekt av inlärningen och beror på korrelationen mellan mönster.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BoltzmannMachines">
		<Question>
			Vad är den typiska lagringskapaciteten för ett Hoptfieldnätverk, och vad kan göras för att förbättra den?
		</Question>
		<Answer>
			Den är [math]0.138N[/math] där [math]N[/math] är antalet noder, under antaganet att antalet 1:or och 0:or är ungefär lika många. För att förbättra denna siffra, så kan man använda sig av mönster som har få aktiva noder (sparse activity), och då får man istället [math]N*log(N)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Vad innebär "djup" inom maskininlärning?
		</Question>
		<Answer>
			[ul]
				[li]Antalet nivåer av sammansättningar av icke-linjära operationer i funktionen som approximeras.[/li]
				[li]Längden av den längsta stigen ifrån indata till utdata i grafen.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Vad för fördelar finns det med en djup struktur?
		</Question>
		<Answer>
			[ul]
				[li]Modellen blir mer kraftfull och kompakt (förbättrar generallisering och minskar antalet frihetsgrader).[/li]
				[li]Inspireras av hjärnans hierarkiska organisation.[/li]
				[li]Kognitiv inspiration - flera nivåer av abstraktion.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Vad för problem finns det med en djup struktur?
		</Question>
		<Answer>
			De är svårare att träna, pga. problemet med vanishing gradents som uppstår i backprop och att det kräver att man gör en icke-konvex optimering.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Hur fungerar inlärning i deep learning nätverk?
		</Question>
		<Answer>
			Den består av två faser:

			[b]Greedy pre-training[/b]
			Här används oövervakad inlärning, där varje lager tränas i taget. Först så tränas det första lagret, med faktiska indata som indata. Sedan tränas det andra lagret, med utdata ifrån det första lagret som input, osv. De vikter som tas fram används i nästa steg.

			[b]Fine tuning[/b]
			Sedan tränas hela nätverket med backprop (med märkt data).

			I första steget så behövs inte märkt data, så oftast är datamängden för första steget mycket störe än i det andra steget. Varje gömt lager kommer att skapa en egen representation av datan, där djupare lager har en högre grad av abstrakation. Exempel är: kanter -> objekt delar -> objekt för bilder.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Vad för fördelar finns det med deep learning jämförelse backprop?
		</Question>
		<Answer>
			[ul]
				[li]Snabbare övervakad träning med mindre chans att fastna i dåliga lokala minimum (bra start värden).[/li]
				[li]Effektiv använding av omärkt data.[/li]
				[li]Problemet med vanishing gradient minskas.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Ange olika algoritmer som används i oövervakade steget i deep learning.
		</Question>
		<Answer>
			[ul]
				[li]Restriected Boltzmann Machine (RBM).[/li]
				[li]Convolutional nerual networks (CNN).[/li]
				[li]Auto-encoders (AE).[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="DeepLearning">
		<Question>
			Vad för olika hypoteser finns det av rollen av pre-training i deep learning?
		</Question>
		<Answer>
			[b]Regularisation[/b]
			Den fungerar som regularisation (dvs en implicit bestraffnings term), som minskar variansen.

			[b]Optimering[/b]
			Den hittar bättre initialvärden för gradient-baserad optimering (t.ex. backprop).
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Vad för typ av nätverk kan användas för att behandla tidsbaserad data?
		</Question>
		<Answer>
			[ul]
				[li]Tapered delay line[/li]
				[li]Low pass filter bank[/li]
				[li]Recurrent backprop[/li]
				[li]Reservoir computing[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Vad för nackdelar finns det för de olika temporal networks?
		</Question>
		<Answer>
			[ul]
				[li]Tapered delay line: du måste anta ett specifikt tidsfönster för data:n.[/li]
				[li]Recurrent backprop: det ger endast länkar mellan datapunkter i följd.[/li]
				[li]Reservoir computing: kräver att flera parameterar behövs definieras i nätverket.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Hur fungerar tapered delay line (för temporal networks)?
		</Question>
		<Answer>
			Indata skickas via fördröjningslinjen (delay line) ett tidssteg i taget. När alla fördröjningslinjer har fått data (dvs att första datapunkten har propageras till den sista fördröjningslinjen), så skickas fördröjningslinjen som input till ett Backprop nätverk. På detta sätt konverteras tid till ett mönster.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Hur fungerar Recurrent backprop (för temporal networks)?
		</Question>
		<Answer>
			Fungerar så att utdata skickas som indata i nätverket.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Hur fungerar low pass filter bank (för temporal networks)?
		</Question>
		<Answer>
	
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Vad är en "reservoir" i "reservoir computing"?
		</Question>
		<Answer>
			Det är en samling av noder, där alla noder är kopplade till varandra (recurrent). Strukturen är ofta slumpmässig och enheterna är ofta icke-linjära (sigmoid). Den övergripande dynamiken för systemet drivs av indata och påverkas också av tidigare tillstånd, och utökar representationen till en högre dimension.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="TemporalNetworks">
		<Question>
			Vad är "readout" i "reservoir computing"?
		</Question>
		<Answer>
			Det är ofta ett enkelt perceptronlager, som t.ex. tränas med: linjär-regression eller Backprop.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ReinforcementLearning">
		<Question>
			Hur fungerar Reinforcement Learning?
		</Question>
		<Answer>
			Det består av en agent och en miljö. Agenten är i ett tillstånd (state) och genom att utföra åtgärder (action), så kommer agenten till ett nytt tillstånd. För varje action som tas, så får agenten en belöning (reward) från miljön. Det som agenten försöker maximera är den totala belöningen.

			Den belöningen som kan förväntas kunna få ett tillstånd givet att en policy [math]pi[/math] följs (dvs vad för action som ska tas i varje tillstånd) beskrivs av värdefunktionen (value-function, [math]V^pi[/math]). 
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ReinforcementLearning">
		<Question>
			Varför behövs ett neuron nätverk för reinforcement learning i praktiken?
		</Question>
		<Answer>
			Om antalet tillstånd och åtgärder är många, så kommer värdefunktionen [math]V(s)[/math] bli för stor att lagra (för det är i princip en mapping från tillstånd till en skalär), och det krävs även ett neuron nätverk för att kunna hantera tillstånd som inte påträffades under inlärning (genom att interpolera mellan närliggande tillstånd).
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ReinforcementLearning">
		<Question>
			Hur sker inlärning i reforcement öearning?
		</Question>
		<Answer>
			Det sker ofta med Temporal-Difference (TD) inlärning. Det fungerar så att man kollar på skillnanden mellan två tillstånd.
		</Answer>
	</TestQuestion>
</Test>
