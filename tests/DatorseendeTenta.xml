<?xml version="1.0" encoding="utf-8" ?>
<Test Name="Tenta">
	<TestQuestion Category="Basic">
		<Question>
			Vad är irradiance ([math]E[/math])?
		</Question>
		<Answer>
			Anger hur mycket ljuset som bestrålar en yta, mäts i watt per kvadrat meter.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Basic">
		<Question>
			Vad är radiance ([math]L[/math])?
		</Question>
		<Answer>
			Anger hur mycket ljus som strålas ut ifrån en yta, mäts i effekt per area per vinkel. Informellt kallas detta för ljusstyrka.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Basic">
		<Question>
			Hur kan man beräkna intensiteten?
		</Question>
		<Answer>
			[math]text(Intensity) = E xx text(area) xx text(exposure time)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Basic">
		<Question>
			Hur omvandlar en kamera verkligheten till en bild?
		</Question>
		<Answer>
			[ul]
				[li]Punkter i värden projiceras till kameras sensorkrets.[/li]
				[li]Kameras samplar irradiance för att beräkna energivärden.[/li]
				[li]
					Positionen i kamera koordinater (mm) omvandlar till bildkoordinater (pixlar) baserat på kamerans intrinsic parametrar:
					[ul]
						[li]Storlek av varje senor element.[/li]
						[li]Bildförhållande.[/li]
						[li]Antal sensor element.[/li]
						[li]Bildens centrum i kretsen relativt linsen.[/li]
					[/ul]
				[/li]
				[li]Samplar sedan en kontinuerlig signal med oändligt antal punkter, och kvantifierar registrerade värdena till ett ändligt antal nivåer.[/li]
				[li]Sampling avstånden [math]Delta x, Delta y, Delta t[/math] anger hur snabbt förändringar kan fångas in.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Basic">
		<Question>
			Vad innebär kvantifiering (quantization)?
		</Question>
		<Answer>
			Tilldelar ett heltal till pixlar (samplar amplituden hos en funktion).
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Basic">
		<Question>
			Vad är kvantifieringsfelet?
		</Question>
		<Answer>
			Skillnaden mellan det reella värdet och de kvantifierade värdet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Basic">
		<Question>
			Vad innebär "saturation"?
		</Question>
		<Answer>
			När det fysiska värdet förflyttas sig utanför intervallet av värden. Detta gör att värdet representeras istället av slutet av intervallet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad är en "perspective projection" för något?
		</Question>
		<Answer>
			Det är en mappning ifrån 3D värld till ett 2D plan.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/PerspectiveProjection.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad är en "pinhole camera" för något?
		</Question>
		<Answer>
			Det är den enklaste typen av avbildningsenhet vilket fångar in geometrin av en perspektiv projektion. Det fungerar så att ljuset kommer in i kameran genom en öppning som är oändligt liten. Skärningen mellan ljusstrålen och bildplanet bildar avbildningen av objektet.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/PinholeCamera.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad är syftet med linser?
		</Question>
		<Answer>
			Syftet är att fånga in ljus ifrån en större öppning (aperture). Problemet med linser är dock att endast ljusstrålar ifrån punkter på fokalplanet skär samma punkt i bildplanet. Resultatet blir att det blir suddigt framför eller bakom bilden. Fokaldjupet anger intervallet av avstånd med acceptabel suddighet.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/Lenses.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Ange olika kamera modeller.
		</Question>
		<Answer>
			[b]Perspective projection (general kamera modell)[/b]
			Alla ljusstrålar konvergerar emot en gemensampunkt - fokalpunkten.

			[b]Orthographic projection (approximation för objekt långt borta)[/b]
			Alla ljusstrålar är vinkelrätta emot bildplanet.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/CameraModels.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Ange ekvationerna för olika typer av projektioner.
		</Question>
		<Answer>
			[b]Perspective projection[/b]
			[math]x/f = X / Z, y / f = Y / Z[/math]

			[b]Orthographic projection[/b]
			[math]x = X, y = Y[/math]

			[b]Scaled orthography[/b]
			[math]x / f = X / Z_0, y / f = Y / Z_0[/math] där [math]Z_0[/math] är det representativa djupet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad är en perspektiv transformation för något?
		</Question>
		<Answer>
			Det är en transformation som består av tre komponenter:
			[ul]
				[li]Rotation: från världen till kamerans koordinatsystem.[/li]
				[li]Translation: från världen till kamerans koordinatsystem.[/li]
				[li]Perspective projection: från kameran till bildkoordinater.[/li]
			[/ul]Dessa egenskaper bevaras:
			[ul]
				[li]Linjer projiceras till linjer[/li]
				[li]Kolinjära egenskaper förbli kolinjära.[/li]
				[li]Tangenta egenskaper förblir tangenta.[/li]
				[li]Skärningar bevaras[/li]
			[/ul]Varje mängd av parallella linjer möts vid en viss punkt - vanishinging point. En mängd av parallella linjer på samma plan leder till kolinjära vanishing points, denna linje kallas för horisonten för planet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad är homogena koordinater?
		</Question>
		<Answer>
			Modellerar punkter [math](X, Y, Z)[/math] i [math]RR^3[/math] av [math](kX, kY, kZ, k)[/math] där [math]k != 0[/math] är godtycklig. Vi anser även att [math](k_1 X, k_1 Y, k_1 Z, k_1)[/math] är ekvivalent med [math](k_2 X, k_2 Y, k_2 Z, k_2)[/math].

			Homogena koordinater innebär att vi ser alla punkter på samma stråle [math](cx, cy, c)[/math] som ekvivalenta, om vi endast vet avbildningen och inte djupet. Det blir också möjligt att modellera punkter vid oändligheten som [math](X, Y, Z, 0)[/math], vilket anger skärningen för parallella linjer.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Hur kan projektionsekvationerna skrivas m.h.a. homogena koordinater?
		</Question>
		<Answer>
			[math]((cx), (cy), (c)) = ((f, 0, 0, 0), (0, f, 0, 0), (0, 0, 1, 0)) ((kX), (kY), (kZ), (k)) = ((fkX), (fkY), (kZ))[/math]

			Bildkoordinater kan fås genom att normalisera så att den tredje komponenten blir 1 (dela med [math]c=kZ[/math]):
			[math]x = (xc) / c = (fkX) / (kZ) = f X / Z, y = (yc) / c = (fkY) / (kZ) = f Y / Z[/math]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Ange hur transformationer ser ut i homogena koordinater.
		</Question>
		<Answer>
			[b]Translation[/b]
			[math]((X), (Y), (Z), (1)) \rightarrow ((1, 0, 0, Delta X), (0, 1, 0, Delta Y), (0, 0, 1, Delta Z), (0, 0, 0, 1)) ((X), (Y), (Z), (1))[/math]

			[b]Skalning[/b]
			[math]((X), (Y), (Z), (1)) \rightarrow ((S_X, 0, 0, 0), (0, S_Y, 0, 0), (0, 0, S_Z, 0), (0, 0, 0, 1)) ((X), (Y), (Z), (1))[/math]

			[b]Rotation runt Z-axeln[/b]
			[math]((X), (Y), (Z), (1)) \rightarrow ((cos \theta, -sin \theta, 0, 0), (sin \theta, cos \theta, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)) ((X), (Y), (Z), (1))[/math]

			[b]Spegling i XY planet[/b]
			[math]((X), (Y), (Z), (1)) \rightarrow ((1, 0, 0, 0), (0, 1, 0, 0), (0, 0, -1, 0), (0, 0, 0, 1)) ((X), (Y), (Z), (1))[/math]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad innebär extrinsic kameraparameterar?
		</Question>
		<Answer>
			Hur kameran är roterad och förflyttad i verkliga världen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad innebär intrinsic kameraparameterar?
		</Question>
		<Answer>
			[ul]
	    		[li]Principal point (mitten av optiska axeln i bilden): [math](u_0, v_0)[/math].[/li]
	    		[li]Brännvidd: [math](f_x, f_y)[/math].[/li]
	    		[li]Hur x och y-axlarna är förvrängda mellan varandra (skew): [math]gamma[/math].[/li]
	    		[li]Linsförvrängning.[/li]
	    	[/ul]Detta sammanfattas av: [math]P = (((f_x), gamma, u_0, 0), (0, (f_y), v_0, 0), (0, 0, 1, 0))[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad är en affine kamera?
		</Question>
		<Answer>
			Det är en linjär approximation av en perspektiv projektion.
			[math]((x), (y), (1)) = ((m_11, m_12, m_13, m_14), (m_21, m_22, m_23, m_24), (0, 0, 0, 1)) ((X), (Y), (Z), (1))[/math].

			De grundläggande egenskaperna är att transformationen är linjär och parallella linjer i 3D avbildas till parallella linjer i 2D. Vinklar bevaras dock ej.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ProjektionerOchTransformationer">
		<Question>
			Vad innebär image warping?
		</Question>
		<Answer>
			Det innebär att man skapar en ny bild [math]g(u, v)[/math] av en bild [math]f(x, y)[/math] genom att transformera [math]u = u(x, y), v = v(x, y)[/math]. Detta görs genom att man samplar ifrån [math]f(x, y)[/math] antingen genom:
			[ul]
				[li]Nearest neighbour look-up (ger brusigt resultat).[/li]
				[li]Bilinear interpolation (ger suddigt resultat).[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grafteori">
		<Question>
			Ange olika typer av "neighbourhoods".
		</Question>
		<Answer>
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/Neighbourhood.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grafteori">
		<Question>
			Vad är en stig (path)?
		</Question>
		<Answer>
			En stig ifrån [math]p[/math] till [math]q[/math] är en mängd av punkter [math]p_0, ..., p_n[/math] så att varje punkt [math]p_i[/math] är en granne till [math]p_(i - 1)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grafteori">
		<Question>
			Vad är en sammanhängande komponent (connected component)?
		</Question>
		<Answer>
			För varje [math]p[/math] så är mängden av alla punkter [math]q[/math] där det finns en stig mellan [math]p[/math] och [math]q[/math] dess sammanhängande komponent.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grafteori">
		<Question>
			Vad innebär en "outer boundary"?
		</Question>
		<Answer>
			Bakgrundspunkter som har en granne som är inom objektet.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/OuterBoundary.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grafteori">
		<Question>
			Ange olika metoder/funktioner av mäta avstånd.
		</Question>
		<Answer>
			[ul]
				[li]Euclidean distance: [math]d(p, q) = sqrt((p.x - q.x)^2 + (p.y - q.y)^2)[/math].[/li]
				[li]City block distance: [math]d(p, q) = |p.x - q.x| + |p.y - q.y|[/math].[/li]
				[li]Chessboard distance: [math]d(p, q) = max(|p.x - q.x|, |p.y - q.y|)[/math].[/li]
			[/ul]Alla dessa metoder uppfyller:
			[ul]
				[li][math]d(p, q) >= 0[/math].[/li]
				[li][math]d(p, q) = d(q, p)[/math].[/li]
				[li][math]d(p, r) le d(p, q) + d(q, r)[/math].[/li]
			[/ul][img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/Distances.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Grafteori">
		<Question>
			Vad är en distanstransform?
		</Question>
		<Answer>
			Det är en typ av avbildning som visar avståndet till närmaste gräns för varje punkt.
		</Answer>
	</TestQuestion>
	<!-- Något om Distance transform for matching shapes? -->

	<TestQuestion Category="LinjäraFilter">
		<Question>
			Vad för egenskaper har en linjär operation?
		</Question>
		<Answer>
			[b]Definition[/b]
			[ul]
				[li]Additivet: [math]L(f(x, y) + g(x, y)) = L(f(x, y)) + L(g(x, y))[/math].[/li]
				[li]Homogenitet: [math]L(alpha f(x, y)) = alpha L(f(x, y))[/math].[/li]
			[/ul]där [math]f(x, y), g(x, y)[/math] är bilder, [math]alpha in RR[/math] och [math]L[/math] en operator.

			[math]L[/math] säges vara shift-invariant om och endast om en shift (translation) av indata resulterar i samma shift i utdatan. Alternativt: [math]L[/math] är kommutativ med en shift operatorn [math]S[/math].
			<!-- [b]Egenskaper speciellt för filter[/b]
			[ul]
				[li]Shift-invariance: om systemet ges två impulser med en tidsfördröjning, så är responsen den samma utom för tidsdifferensen.[/li]
				[li]Signaler kan representeras som summor av impluser av olika styrkor (bildintensitet), förskjutna i tiden (bildrymden)[/li]
				[li]Impulse-response function: Om vi vet hur systemet reagerar till en implus, så vet vi hur den reagerar till en kombination av impluser.[/li]
			[/ul] -->
			<!-- [b]Shift-invariant[] -->
		</Answer>
	</TestQuestion>
	<TestQuestion Category="LinjäraFilter">
		<Question>
			Hur representeras och appliceras ett linjärt filter?
		</Question>
		<Answer>
			Den representeras av en filter kernel, och appliceras m.h.a faltning (convolution) på bilden.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="LinjäraFilter">
		<Question>
			Hur är faltning definierad?
		</Question>
		<Answer>
			[b]Kontinuerlig[/b]
			[math]f(x) ** g(x) = int_(alpha in R^n) f(alpha) g(x - alpha) d alpha[/math].
			Faltning är också kommutativ: [math]f(x) ** g(x) = g(x) ** f(x) = int_(alpha in R^n) g(alpha) f(x - alpha) d alpha[/math].

			[b]Diskret[/b]
			Faltningen av en bild [math]f(x, y)[/math] med en kernel [math]h(x, y)[/math] definieras som:
			[math]g(x, y) = h(x, y) ** f(x, y) = sum_(m=-M)^M sum_(n=-N)^N h(m, n) f(x - m, y - n)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="LinjäraFilter">
		<Question>
			Vad för relation finns det med en shift-invariant operator och faltning?
		</Question>
		<Answer>
			Varje shift-invariant operator kan skrivas som en faltning [math]L(f) = g ** f[/math].

			[b]Kontinuerlig[/b]
			[math]L(f(x)) = int_(alpha in R^n) g(alpha) f(x - alpha) d alpha[/math]

			[b]Diskret[/b]
			[math]L(f(x)) = sum_(alpha in R^n) g(alpha) f(x - alpha)[/math]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad är en Fourier transformation för något?
		</Question>
		<Answer>
			Det är en formel för att beräkna frekvens, amplitud och fasen för varje sinusvåg som behövs för att representera en given signal. Fourier transformationen konverterar en signal (bild) mellan spatial och frekvensdomänen.
	<!-- 		I Fourier rymden, så representerar varje punkt en specfik frekvens som förvarades i den spatiala domänen. -->
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad för för- och nackdelar finns det för en spatial representation och en frekvens representation?
		</Question>
		<Answer>
			Notera: båda representationerna innehåller lika mycket information.
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/SpatialVsFourie.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad anger den spatiala frekvensen?
		</Question>
		<Answer>
			Den anger med vilken hastighet som pixel intensiteten förändras.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad för relation finns det mellan basbyte och Fourier transformationen?
		</Question>
		<Answer>
			Fourier transformationen kan ses som ett basbyte, där baserna i den spatiala domänen är shiftade Dirarc funktioner och i Fourier domänen komplexa exponentialfunktioner.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Ange ekvationerna för Fourier transformationen.
		</Question>
		<Answer>
			[b]Kontinuerlig[/b]
			[math]cc "F"(f(x)) = int_(x in R^n) f(x) e^(- i omega^T x) dx = hat(f) (omega)[/math]
			[math]cc "F"^-1(hat(f) (x)) = 1 / (2 pi)^n int_(omega in R^n) hat(f) (x) e^(i omega^T x) d omega[/math]
			[math]e^(i omega^T x) = cos(omega^T x) + i * sin(omega^T x)[/math]

			Terminologi:
			[ul]
				[li]Frequency spectrum: [math]hat(f)(omega) = Re(omega) + i Im(omega) = |hat(f)(omega)| e^(i phi(omega))[/math].[/li]
				[li]Fourier spectrum: [math]|hat(f)(omega)|=sqrt(Re^2(omega) + Im^2(omega))[/math].[/li]
				[li]Power spectrum: [math]|hat(f)(omega)|^2[/math].[/li]
				[li]Phase angle: [math]phi(omega)=arg hat(f) (omega) = tan^-1 ((Im(omega)) / (Re(omega)))[/math].[/li]
				[li]Angular frequency: [math]omega = (omega_1, omega_2)^T[/math], [math]omega_1[/math] i x riktning, [math]omega_2[/math] i y riktning.[/li]
				[li]Frequency: [math]f = omega / (2 pi)[/math].[/li]
				[li]Wavelength: [math]lambda = (2 pi) / ||omega||[/math].[/li]
			[/ul]Alternativ representation: Fourie koefficienterna [math]hat(f) (omega_1, omega_2)[/math] är komplexa tal, med det är inte uppenbart vad reella och imaginära delarna representerar. Ett annat sätt är att representera det är med fas och magnitud.
			[ul]
				[li]Magnitud: [math]|hat(f)(omega_1, omega_2)| = sqrt(Re^2(omega_1, omega_2) + Im^2(omega_1, omega_2))[/math].[/li]
				[li]Fas: [math]phi(omega_1, omega_2) = tan^-1 ((Im(omega_1, omega_2)) / (Re(omega_1, omega_2)))[/math].[/li]
			[/ul]Det gäller även att sinusoids och cosinusoids är egenfunktioner av faltningar.

			[b]Diskret[/b]
			[math]hat(f)(u, v) = 1 / sqrt(MN) sum_(m=0)^(M-1) sum_(n=0)^(N-1) f(m, n) e^(-2 pi i ((m u) / M + (n v) / N))[/math]
			[math]f(m, n) = 1 / sqrt(MN) sum_(u=0)^(M-1) sum_(v=0)^(N-1) hat(f)(u, v) e^(+2 pi i ((m u) / M + (n v) / N))[/math]

			Terminologi:
			[ul]
				[li]Fourier spectrum: [math]|F(u, v)|=sqrt(Re^2(u, v) + Im^2(u, v))[/math].[/li]
				[li]Phase angle: [math]phi(u, v) = tan^-1 ((Im(u, v)) / (Re(u, v)))[/math].[/li]
				[li]Power spectrum [math]P(u, v) = |F(u, v)|^ 2 = Re^2(u, v) + Im^2(u, v)[/math].[/li]
				[li]Magnitude: är största värdet.[/li]
				[li]Phase: anger origo, där sinusoiden börjar.[/li]
			[/ul][b]Relation[/b]
			I 1D så ges relationen mellan diskret och kontinuerlig av [math]omega = (2 pi u) / M[/math]. Detta innebär att [math]u in [0, M - 1][/math] och [math]omega in [0, 2 pi)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad säger faltningssatsen?
		</Question>
		<Answer>
			Att faltning i den spatiala domänen är samma sak som multiplikation i Fourier domänen (omvänt gäller också):
			[math]cc "F" (h ** f) = cc "F"(h) * cc "F"(f), cc "F"(f * g) = cc "F"(f) ** cc "F"(g)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad innebär det att ett filter är separerbar?
		</Question>
		<Answer>
			Det innebär att en kernel i 2D [math]h(x, y)[/math] kan delas som en serie av 1D faltningar.
			Dvs: Om [math]hat(f)(u, v) = 1 / sqrt(M N) sum_(m=0)^(M-1) sum_(n=0)^(N-1) f(m, n) e^(-2 pi i ((m u) / M + (n v)/N))[/math] så kan den skrivas som: [math]hat(f)(u, v) = 1 / sqrt(M) sum_(m=0)^(M-1) (1 / sqrt(N) sum_(n=0)^(N-1) f(m, n) e^(-2 pi i (n v) / N)) e^(-2 pi i (m u) / M)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Ange olika egenskaper för Fourier transformationen.
		</Question>
		<Answer>
			[b]Linjäritet[/b]
			[math]cc "F" [a f_1(m, n) + b f_2(m, n)] = a hat(f)_1(u, v) + b hat(f)_2(u, v)[/math]
			[math]a f_1(m, n) + b f_2(m, n) = cc "F"^-1 [a hat(f)_1(u, v) + b hat(f)_2(u, v)][/math]

			[b]Modulation[/b]
			[math]cc "F" [f(m, n) * e^(2 pi i ((m u_0) / M + (n v_0) / N))] = hat(f)(u - u_0, v - v_0)[/math]. Detta innebär att origo för Fourie transformen kan flytas till mitten av bilden genom att multiplicera med [math](-1)^(m + n)[/math].

			[b]Translation[/b]
			Om bilden flyttas, så genomgår Fourier spektrumet en fasförskjutning, men magnituden av spektrumet är den samma.
			[math]cc "F"[f(m - m_0, n - n_0)] = hat(f)(u, v) e^(-2 pi i ((m_0 u) / M + (n_0 v) / N))[/math]
			[math]|hat(f)(u, v) e^(-2 pi i ((m_0 u) / M + (n_0 v) / N))| = |hat(f)(u, v)|[/math]

			[b]Rotation[/b]
			Om bilden roteras med en vinkel [math]alpha[/math], så roteras Fourier transformationen med samma vinkel.

			[b]Skalning[/b]
			Komprimering i den spatiala domänen är samma sak som expansion i Fourie domänen (och tvärtom).

			[b]Periodisk[/b]
			Den diskreta Fourier transformen och dess invers är periodisk, med en period [math]N[/math] för en [math]N xx N[/math] bild. Alltså: [math]hat(f)(u, v) = hat(f)(u + N, v) = hat(f)(u, v + H) = hat(f)(u + N, v + N)[/math]. Detta innebär att [math]hat(f)[/math] återupprepar sig oändligt. Det krävs dock endast en period för att återskapa original funktionen [math]f[/math].

			[b]Konjugat symmetri[/b]
			Fourier transformen uppfyller [math]hat(f)(u, v) = hat(f)^(**)(-u, -v)[/math] och [math]|hat(f)(u, v)|=|hat(f)(-u, -v)|[/math]. Om vi använder att [math]hat(f)[/math] är periodiskt och är symmetriskt runt origo, får vi att vi inte behöver [math]2N^2[/math] (hälften för reella delen och hälften för imaginära) värden för att representera en [math]N xx N[/math] bild i Fourier domänen, utan [math]N^2[/math] om vi använder symmetrin.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Ange våglängden och perioden för en 2D sinusoid.
		</Question>
		<Answer>
			[img width="50%" height="50%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/Wavelength.png[/img]
		</Answer>
	</TestQuestion>
	<!-- Någon on transfer funktioner? -->
	<TestQuestion Category="FourierTransform">
		<Question>
			Ange vad sampling innebär för Fourier transformer.
		</Question>
		<Answer>
			Signalerna (bilderna) är ej kontinuerliga, men diskreta. En kontinuerlig funktion [math]f(x, y)[/math] kan då samplas av ett diskret rutnät av sampling punkter.
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/Sampling.png[/img]
			Där [math]Delta x, Delta y[/math] kallas för sampling intervallen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Hur kan sampling beskrivas mer formellt?
		</Question>
		<Answer>
			Vi beskriver den ideala impulsen m.h.a Dirarc distributionen:
			[math]int_(-\infty)^(infty) int_(-infty)^(infty) delta(x, y) dx dy = 1[/math] och [math]delta(x, y) = 0[/math] för alla [math]x, y != 0[/math].

			"Sifting" egenskapen av Dirac funktionen ger då ett värde för funktionen [math]f(x, y)[/math] vid punkten [math](a, b)[/math]: [math]int_(-\infty)^(infty) int_(-infty)^(infty) f(x, y) delta(x - a, y - b) dx dy = f(a, b)[/math].

			Den ideala samplingen [math]s(x, y)[/math] i ett rutnät kan då beskrivas som en samling av Dirac funktioner [math]delta[/math]: [math]s(x, y) = sum_(j=1)^M sum_(k=1)^N delta(x - j Delta x, y - k Delta y)[/math]. Då är den samplade bilden [math]f_s(x, y)[/math] en produkt av den kontinuerliga bilden [math]f(x, y)[/math] och sampling funktionen [math]s(x, y)[/math].

			[math]f_s(x, y) = f(x, y) s(x, y)[/math]
			[math] = f(x, y) sum_(j=1)^M sum_(k=1)^N delta(x - j Delta x, y - k Delta y)[/math]
			[math] = sum_(j=1)^M sum_(k=1)^N  f(j Delta x, k Delta y) delta(x - j Delta x, y - k Delta y)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad för fel kan uppstå vid sampling?
		</Question>
		<Answer>
			[ul]
				[li]Intensity quantization: inte tillräckligt med upplösning för intensiteten.[/li]
				[li]Spatial aliasing: inte tillräckligt med spatial upplösning.[/li]
				[li]Temporal aliasing: inte tillräckligt med temporal upplösning.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad är aliasing?
		</Question>
		<Answer>
			Det är artefakter som produceras av under-sampling eller en dålig omkonstruktion. Detta innebär att bilden ser pixelig ut. Detta löses med anti-aliasing.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Ange olika sätt att göra anti-aliasing
		</Question>
		<Answer>
			[ul]
				[li]Öka hastigheten vi samplar med (dock inte möjligt i praktiken)[/li]
				[li]Minska högsta frekvensen förre sampling, vilket görs med blurring.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FourierTransform">
		<Question>
			Vad säger sampling satsen?
		</Question>
		<Answer>
			[ul]
				[li]En signal säges vara band begränsad om dess högsta frekvens är begränsad. Denna frekvens kallas för bandbredden (bandwidth).[/li]
				[li]Sinus/cosinus komponenten av den högsta frekvensen anger den högsta frekvens innehållet för signalen.[/li]
				[li]Om signalen samplas med en hastighet som är samma, eller mer än två gånger så stor som den högsta frekvensen, så kan original signalen återskapas komplett från samples.[/li]
				[li]Den lägsta sampling hastigheten för en band begränsad funktion kallas för dess Nyquist hastighet.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<!-- degradation vs restoration -->
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad för krav brukar vi ställa på en transformation [math]s = T(r)[/math] (av pixelvärden)?
		</Question>
		<Answer>
			[ul]
				[li][math]T(r_text(min))=r_text(min)[/math], [math]T(r_text(max))=r_text(max)[/math] (eller omvänt). Detta innebär att vi fyller upp hela intervallet av värden.[/li]
				[li][math]T[/math] är monotonisk, vilket innebär att den också är inverterbar (vi tappar ingen information).[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär en Look-Up Table (LUT)?
		</Question>
		<Answer>
			Det innebär att vi har sparat en tabell, som mappar ifrån input till utvärdet, istället för att beräkna funktionen direkt.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär en log transformation?
		</Question>
		<Answer>
			Det innebär att vi applicerar funktionen [math]s = c log (1 + r)[/math]. Den är användbar för att komprimera stor "dynamic range" och för att göra detaljer synliga.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär power-law transformation?
		</Question>
		<Answer>
			Det innebär att vi applicerar funktionen [math]s = c r^gamma[/math] eller [math]s = c (r + epsilon)^gamma[/math]. Många enheter som används för kameror, utskrift och skärmar lyder under en sådan lag.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär histogram stretching?
		</Question>
		<Answer>
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/HistogramStretching.png [/img]

			Det innebär att vi ökar konstrasten genom att låta intervallet [math][c, d][/math] täcka hela bilden. Detta innebär att vi tappar information i området [math][a, b][/math] och [math][d, b][/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär histogram equalization?
		</Question>
		<Answer>
			Idén är att vi omdistribuera gråskala värdena så jämnt som möjligt. Detta skulle motsvara en distribution av ljussättning där alla värden är lika sannolika.

			Detta går till så att vi först beräknar ett histogram över pixelvärden för bilden. Sen sparar vi den kumulativa summan av alla histogram värden och normaliserar varje värde genom att dela med antalet pixlar. Detta använder vi sen för att transformera bilden.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Ange olika typer av linjära filter som används för att förbättra bilder.
		</Question>
		<Answer>
			[ul]
				[li]Low-pass: eliminera komponenter med hög frekvens (kanter och skarpa detaljer). Detta leder till att bilden blir suddig.[/li]
				[li]High-pass: eliminera komponenter med låg frekvens (saker som varierar sakta, t.ex. skuggning). Detta leder till att kanter och andra detaljer (även brus) blir mer skarpa.[/li]
				[li]Bandpass: eliminera komponenter som ligger utanför ett frekvensområdet. Detta leder en kombination av low- och highpass.[/li]
			[/ul][img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/LinearFiltersImageEnhancement.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Ange olika typer av brus.
		</Question>
		<Answer>
			[ul]
				[li]Signal oberoende additivt brus (sampling brus): [math]g = f + nu[/math].[/li]
				[li]Signal beroende multiplikativt brus (ljussättnings variation): [math]g = f + nu f = (1 + nu) f[/math].[/li]
				[li]Uppmättningsbrus (salt och peppar)[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Hur fungerar mean filtering?
		</Question>
		<Answer>
			Detta innebär att vi beräknar ett nytt pixelvärde som ett medelvärde för ett område runt pixeln. Detta görs lättast med en kernel, t.ex.: [math]1 / 9 [[1, 1, 1], [1, 1, 1], [1, 1, 1]][/math]. Själva kerneln kan variera, men den bör uppfylla:
			[ul]
				[li]Koefficienterna summerar till 1.[/li]
				[li]Symmetriskt upp/ner och vänster/höger.[/li]
				[li]Center pixeln har störst inverkan.[/li]
				[li]Filtret bör vara separerbar.[/li]
			[/ul]Ett filter som uppfyller detta är: [math]C=(((Delta t) / 2), (1 - Delta t), ((Delta t) / 2)) (((Delta t) / 2, 1 - Delta t, (Delta t) / 2))[/math]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad är ett ideellt low pass filter?
		</Question>
		<Answer>
			Det är en filter som är ideellt i frekvensdomänen, men inte ideellt i den spatiala domänen. Detta ger ringar/suddiga bilder.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad är ett Gaussian low-pass filter?
		</Question>
		<Answer>
			Det är ett filter som använder en normalfördelning, med kvadrerade avståndet ifrån origo. En lägre varians ger en högre "cutoff" frekvens och en mildare filtrering.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad är en Binomial kernel?
		</Question>
		<Answer>
			Det är en typ av filter som använder sig av binomialkoefficienter. När vi ökar storleken, så närmar vi oss en Gaussian kernel.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär Anisotropic smoothing?
		</Question>
		<Answer>
			Det innebär att vi smoothar olika i olika riktningar, för att bevara kanter. Idén är att vi smoothar pixlar baserad på en likhet mellan pixlar, där det t.ex. kan vara i färg eller position.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Vad innebär median filtering?
		</Question>
		<Answer>
			Det innebär att vi filtrerar baserat på medianen i ett område. Fördelarna är att den bevarar skuggor, skarpa kanter, eliminerar lokala extremvärden (t.ex. salt och peppar brus), men ger att bilden ser ut som en målning.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Hur kan vi beräkna derivatan av en bild?
		</Question>
		<Answer>
			[b]Första ordningen[/b]
			[math]f_x = f(x + 1, y) - f(x, y)[/math]. Detta representeras av [math][[0, -1, 1]][/math]. Men i praktiken så används centraldifferens: [math]f_x = 1 / 2 (f(x + 1, y) - f(x - 1, y)) => 1/2 [[-1, 0, 1]][/math].

			[b]Andra ordningen[/b]
			[math]f_(x x) = f(x + 1, y) + f(x - 1, y) - 2f(x, y) => [[1, -2, 1]][/math].

			[b]Gradienten[/b]
			[math]nabla f = (f_x, f_y)[/math]

			[b]Magnituden av Gradienten[/b]
			[math]|nabla f| = sqrt(f_x^2 + f_y^2)[/math]

			[b]Laplacian[/b]
			[math]nabla^2 f = f_(x x) + f_(yy)[/math]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="Bildförbättring">
		<Question>
			Hur kan vi göra en bild skarpare?
		</Question>
		<Answer>
			[b]Unsharp masking[/b]
			Idén är att vi vill subtraherar bort suddigheten ifrån bilden. Låt [math]bar(f)(x, y)[/math] vara en suddig version av bilden. Då fungerar det så att: [math]g(x, y) = f(x, y) + alpha(f(x, y) - bar(f)(x, y))[/math].

			[b]Highpass filtering[/b]
			[math]G(u, v) = F(u, v) + alpha(H_text(hp)(u, v) F(u, v))[/math]
			där [math]H_text(hp)(u, v)[/math] kan vara:
			Ideellt: [math]H_text(hp)(u, v) = { (1, text(om ) (u^2+v^2) > D_0^2), (0, text(annars)) :}[/math].
			Gaussian: [math]H_text(hp)(u, v) = 1 - e^((-sigma^2 (u^2+v^2))/2)[/math].

			[b]Derivering[/b]
			M.h.a Laplacian: [math]g(x, y) = f(x, y) - nabla^2 f(x, y)[/math]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Vad används Hough transformen till?
		</Question>
		<Answer>
			Den används för att hitta alla möjliga linjer för en mängd av kanter. En linjes styrka beror på hur många punkter som ligger på den linjen.

			Vi introducerar då ackumulator rymden, som innehåller antalet röster för varje linje. Rymden består av linjer på formen: [math]L(k)=-kx+y[/math]. Men på grund av att vertikala linjer representeras av linjer med [math]k \rightarrow pm \infty[/math] använder vi istället en annan representation: [math]x cos(theta) + y sin(theta) = r[/math] där [math]r[/math] är det vinkelrätta avståndet ifrån origo och [math]theta[/math] vinkeln mellan vektorn [math]bar r[/math] och origo.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/HoughTransform.png[/img]

			Hough transformen kan även utökas för att hitta cirklar (3D) och ellipser (5D). Men detta blir mycket ineffektivt att beräkna.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Vad är RANSAC?
		</Question>
		<Answer>
			RANSAC står för Random Sampling Consensus, och likt Hough transformen kan den hitta linjer, cirklar, ellipser, etc.

			[ol]
				[li]Välj slumpmässigt ut ett minimalt antal punkter, 2 för linjer, 3 för cirklar och 5 för ellipser.[/li]
				[li]Beräkna en modell (t.ex. [math]y=kx+L[/math]) ifrån punkterna.[/li]
				[li]Beräkna antalet punkter som är nära till modellen.[/li]
				[li]Upprepa steg 1-3 [math]S[/math] gånger.[/li]
				[li]Välj den lösning med flest antalet matchande punkter.[/li]
			[/ol]För att avgöra hur många iterationer [math]S[/math] som behövs kan man resonera följande: Hur många iterationer behövs för att med en sannolikhet [math]P[/math], om [math]p[/math] % av punkterna tillhör formen? Då får man formeln: [math]S = log(1 - P) / log(1 - p^K)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Vad innebär homografi (homography)?
		</Question>
		<Answer>
			Två bilder ifrån samma plana yta är relaterad genom en [i]homografi[/li]. Detta kan användas för att transformera en punkt i en kamera till en annan kamera, om punkterna ligger på ett gemensamt plan.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Vad för olika typer invarianter vill vi ha för en representation av former?
		</Question>
		<Answer>
			[ul]
				[li]Förflyttning.[/li]
				[li]Skala.[/li]
				[li]Rotation (dock inte alltid, t.ex. 6 och 9).[/li]
				[li]Spegling.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Ange olika enklare representationer av former.
		</Question>
		<Answer>
			[ul]
				[li]Sammanhängande komponenter, med egenskaperna: storlek, bounding box, center-of-gravity.[/li]
				[li]Compactness: [math]text(area)/text(circumference)^2[/math.][/li]
				[li]Eccentricity: [math]text(length of maximum chord A)/text(length of maximum chord B⊥A)[/math].[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Vad är moments?
		</Question>
		<Answer>
			[math]m_(pq)=int int_(Omega) x^p y^q f(x, y) dx dy[/math] där [math]f(x, y) = { (1, text(inom regionen)), (0, text(annars)) :}[/math].
			Vi har också centered moments [math]mu_(pq) int int_(Omega) (f - bar(x))^p (y - bar(y))^q f(x, y) dx dy[/math] med center of gravity: [math]bar(x) = m_10 / m_00, bar(y) = m_01 / m_00[/math].

			Notera att [math]m_00[/math] anger arean för regionen.
			<!-- Något om ellips approxmation? -->
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ShapeRepresentation">
		<Question>
			Vad är PCA?
		</Question>
		<Answer>
			PCA står för [i]principal components analysis[/i], och används för att reducera dimensionen på datan. Idén bakom PCA är att vi antar att den största delen av information finns i de riktningar som varierar mest.

			Låt [math]K[/math] vara dimensionen för den nya datan och [math]N[/math] dimensionen för original datan.

			[b]Hitta en bas[/b]
			Antag att [math]x_1, x_2, ..., x_M[/math] är [math]N xx 1[/math] vektorer.
			[i]Steg 1[/i]
			[math]bar(x) = 1 / M sum_(i=1)^M x_i[/math]

			[i]Steg 2[/i]
			[math]Phi_i = x_i - bar(x)[/math]

			[i]Steg 3[/i]
			Bilda: [math]A = [[Phi_1, Phi_2, ..., Phi_M]][/math].
			Beräkna [math]C = 1 / M sum_(i=1)^M Phi_i Phi_i^T A A^T[/math] (kovariansmatris, [math]N xx N[/math])

			[i]Steg 4[/i]
			Beräkna egenvärdena av [math]C[/math]: [math]lambda_1 > lambda_2 > ... > lambda_N[/math].

			[i]Steg 5[/i]
			Beräkna egenvektorerna av [math]C[/math]: [math]u_1, u_2, ..., u_n[/math]. Dessa vektorer bildar en bas för [math]x - bar(x)[/math].
			[math]x - bar(x) = b_1 u_1 + b_2 u_2 + ... + b_N u_N = sum_(k=1)^N b_k u_k, b_k = u_k^T (x - bar(x))[/math]

			[i]Steg 6[/i]
			Behåll endast de termer med de [math]K[/math] högsta egenvärdena.
			[math]x - bar(x) = sum_(k=1)^K b_k u_k[/math]

			[b]Approximera datan[/b]
			Detta görs genom att projicera vektorerna till den lägre dimensionen: [math]bar(x) = b_1 u_1 + b_2 u_2 + ... + b_K u_K[/math].

			Detta fungerar även för bilder, om vi gör om varje bild till en lång vektor istället.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Vad innebär bildsegmentering (image segmentation)?
		</Question>
		<Answer>
			Det innebär att vi delar in bilden i semantiskt meningsfulla regioner.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Varför bör man ta hänsyn till Spatial coherence?
		</Question>
		<Answer>
			Metoder som baseras på histogram tar ej hänsyn till beroendet mellan närliggande pixlar. Detta kan leda till segment delas upp i olika delar. Om man tar hänsyn till spatial sammanhållning mellan pixlarna, så kan detta undvikas
		</Answer>
	</TestQuestion>
	<!-- Något om Gestalt Theory? -->
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar thresholding?
		</Question>
		<Answer>
			Det fungerar så att vi sätter en gräns, och alla värden som är över gränsen tillhör ena segment, alla värden under gräsen, det andra segmentet. För att automatiskt hitta en gräns så kan vi använda oss av olika metoder:

			[b]P-tile[/b]
			Använd tidigare information om storleken av objektet, antag att objektet har en [math]P %[/math] storlek av hela bilden. Välj då gränsen så att [math]P %[/math] av histogrammet täcks.

			[b]Mode[/b]
			Hitta lokala maximum och minimum i bilden. Sätt gränsen att vara pixelvärdet av dessa maximum/minimum.

			[b]Iterativ[/b]
			Starta med en approximativ gräns och förbättra det iterativt genom att ta hänsyn till någon typ av betygsättning, t.ex.: [math]T = (r_1 + r_2) / 2[/math] där [math]r_i[/math] är medelvärdet av den tidigare segmenterade regionen [math]i[/math].

			[b]Adaptive[/b]
			Dela upp bilden i delbilder, och bestäm en gräns för varje delbild.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar K-means?
		</Question>
		<Answer>
			Den grupperar pixlar baserat på likhet (t.ex. färg).
			[ol]
				[li]Välj [math]K[/math] antal initialvärden för klustren.[/li]
				[li]Tilldela för varje pixel de kluster som den är närmast.[/li]
				[li]Uppdatera centret för klustret m.h.a tilldelade pixlarna.[/li]
				[li]Fortsätt tills dess att kluster centren konvergerar.[/li]
			[/ol]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar mean-shift segmentering?
		</Question>
		<Answer>
			Vi försöker gruppera pixlarna baserat både på färg och position. Vi låter varje pixel representeras av en 5D vektor: [math]bar(x) = [[x, y, R, G, B]][/math]. För att hantera att varje pixel är brusig, så placerar vi en "skål" runt varje pixel: [math]K(x-x_i)[/math].

			Vi vill hitta maximum av: [math]f(x) = 1 / n sum_(i=1)^n K(x-x_i)[/math] där [math]K(x) = C K(||x||^2)[/math]. Då beräknar vi gradienten som: [math]nabla f(x) = C / n sum_(i=1)^n (x-x_i) k'(||x-x_i||^2)[/math]. Vi sätter den till 0 och får då [math]x^text(new) = (sum_(i=1)^n x_i k' (||x-x_i||^2)) / (sum_(i=1)^n k'(||x-x_i||^2))[/math].

			Om vi startar på en specifik pixel, och utför ovan, så konvergerar mot någon av bildens mode. Klustra bilden baserat på vilket mode som punkten konvergerar emot.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur kan man förbättra en segmentering?
		</Question>
		<Answer>
			Genom att slå ihop regioner som är lika och är grannar. Det går även att dela på regioner, om variationen är för stor inom regionen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar watershedding?
		</Question>
		<Answer>
			[ol]
				[li]Skapa en topologiskt karta över bilden (t.ex. med magnituden av gradienten eller distanstransform)[/li]
				[li]Gradvis fyll i "bassänger" med vatten, börja med den djupaste först.[/li]
				[li]När två "bassänger" möts, skapa en kant mellan dessa två segment.[/li]
				[li]Avsluta när varje pixel antigen är fylld eller är en kant.[/li]
			[/ol]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur kan grafteori användas för bildsegmentering?
		</Question>
		<Answer>
			Vi låter varje pixel representera ett hörn i grafen. Pixlar som är grannar är sammankopplade med kanter, och dessa kanter har vikter. Vikterna beror vad för något som algoritmen kollar efter som t.ex. skillnad i intensitet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Vad innebär en affinity matris?
		</Question>
		<Answer>
			Vi bildar en matris, där varje element [math](p, q)[/math] anger hur lika två punkter [math]p[/math] och [math]q[/math] är. Man kan t.ex. kolla på skillnad i intensitet, avstånd eller färg.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar Normalized cuts?
		</Question>
		<Answer>
			Vi vill minimera följande funktion: [math]Ncut(A, B) = (cut(A, B)) / (assoc(A, V)) + (cut(A, B)) / (assoc(B, V))[/math].

			Låt [math]A[/math] och [math]B[/math] vara två disjunkta mängder.

			Då anger [math]cut(A, B)[/math] summan av vikterna för kanterna i snittet mellan [math]A[/math] och [math]B[/math], och [math]assoc(A, V)[/math] summan av vikterna inom mängden [math]A[/math], och på samma sätt för [math]assoc(B, V)[/math].

			Segmentering kan då hittas genom att lösa ett generaliserat egenvärdesproblem.

			Det som algoritmen optimerar efter är likheten inom varje mängd ([math]A, B[/math]) ska bli så stor som möjligt, medans den minimerar likheten mellan kanter i snittet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Vad är en "active contour model"?
		</Question>
		<Answer>
			Vi vill representera en form av en kurva: [math]X(s) = (x(s), y(s)), s in [0, 1][/math]. Vi vill oftast i praktiken också en stängd kurva, [math]X(0) = X(1)[/math]. För att hitta kurvan, försöker vi minimera energi (kostnads) funktion: [math]E = E_text(internal) + E_text(image)[/math].

			Den interna energin försöker göra kurvan så kort som möjligt, [math]E_text(internal) = int alpha(s) ||X_s(s)||^2 + beta(s) ||X_(ss)(s)||^2 ds[/math], där [math]alpha(s), beta(s)[/math] kontrollerar formen vid olika punkter.

			Bildens energi försöker maximera gradienten längs kurvan, [math]E_text(image) = E_text(image) = - int ||nabla I(X(s))||^2 ds[/math].

			Vi diskretiserar sedan genom att dela in konturen i delar. Vi itererar sedan genom att röra sig längs normalerna för att gradvis minska energin.

			[b]Fördelar[/b]
			[ul]
				[li]Lätt att lägga till egen energi formulering beroende på problemet.[/li]
				[li]Kan get mycket bra resultat på vissa problem.[/li]
				[li]Kan utökas till för specifika former.[/li]
			[/ul][b]Nackdelar[/b]
			[ul]
				[li]Kan lätt fastna på felaktiga kanter.[/li]
				[li]Svårt att undvika att den skär sig själv.[/li]
				[li]Kan ej byta topologi.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar "level set" segmentering?
		</Question>
		<Answer>
			Vi skriver kurvan som en funktion av tiden [math]t[/math]: [math]X(s, t) = (x(s, t), y(s, t))[/math]. Vi låter sen kurvan röra sig längs normalen [math]N[/math] med en hastighet [math]k[/math], där [math](del X) / (del t) = k N [/math].

			Vi låter sen kurvan [math]X(s, t)[/math] bädas in i level set funktionen [math]phi(X(s, t), t) = 0[/math]. Vi får sedan att [math]k = nabla N = (phi_(x x) phi_y^2 - 2 phi_(x y) phi_x phi_y + phi_(y y) phi_x^2) / (phi_x^2 + phi_y^2)^(3/2)[/math].

			[b]Fördelar[/b]
			[ul]
				[li]Lätt att lägga till egen funktion beroende på problemet.[/li]
				[li]Kan hantera komplexa former.[/li]
				[li]Kan lätt utökas till flera dimensioner.[/li]
				[li]Tillåter förändringar i topologi.[/li]
			[/ul][b]Nackdelar[/b]
			[ul]
				[li]Konvergerar oftast sakta.[/li]
				[li]Kan vara svår att implementera.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Hur fungerar "graph cut" segmentering?
		</Question>
		<Answer>
			Vi skapar först en graf med ett hörn per pixel, och kanter mellan alla grannar. Vi antar sedan att vi har en mängd av modeller, t.ex.: [math]L = {text(foreground), text(background)}[/math]. Varje pixel tillhör då en modell, vi betecknar detta med [math]I_x[/math]. En modell representeras sedan av en probabilistisk modell, som tränas på en stor databas.

			Vi försöker sedan minimera energi: [math]E = sum_x psi_x(I_x) + sum_x sum_(y in text(neighbors)(x)) psi_(x, y)(I_x, I_y)[/math]. [math]psi_x(I_x)[/math] har låg kostnad om pixeln är likt modellen [math]I_x[/math], annars hög. [math]psi_(x, y)(I_x, I_y)[/math] är låg om pixlarna har olika färger och hög kostnad om de har samma färg. Vi har även om [math]I_x = I_y[/math], så är [math]psi_(x, y)(I_x, I_y)=0[/math].

			Grafsnitt kan användas för minimera energi för fallet där [math]|L|=2[/math]. Detta gör vi genom att lägger till "source" (foreground) och "drain" (background) med kostnader givna av [math]psi_x(I_x)[/math]. Vi hittar sedan den kostnad som delar upp grafen i två delar.
 		</Answer>
	</TestQuestion>
	<TestQuestion Category="ImageSegmentation">
		<Question>
			Vad innebär morphology?
		</Question>
		<Answer>
			Det innebär att vi "rensar" upp en segmentering. En morphological operator definieras av en kernel och en mängd operator. Om två mängder av element matchar villkoret definierat av mängd operatorn, så sätts värdet i centret av kernel (kallat för stukturelementet) till ett fördefinierat värdet (0 eller 1 för binära bilder). Exempel på operatorer är:

			[b]Diltation[/b]
			Den grundläggande effekten blir att gränser för regioner blir större, medans håll i regioner blir mindre. Låt [math]A, B in R^2[/math]. Då blir operatorn: [math]A o+ B = { c in R^2 : c = a + b }[/math]. Typisk är [math]A[/math] en binär bild, medans [math]B[/math] är masken (stukturelementet). Detta kan sammanfattas som: Om minst en pixel i strukturelementet sammanfaller med en pixel i förgrunden i bilden, så sätts input pixeln till värdet för förgrunden.

			[b]Erosion[/b]
			Den grundläggande effekten blir att gränser för regioner blir mindre. medans håll i regioner blir större. Operatorn är: [math]A &#8854; B = { c in R^2 : c + b in A, AAb in B }[/math]. Om för varje pixel i strukturelementet, om pixeln i bilden är i förgrunden, så förblir pixel som den är. Om någon av pixlarna är i bakgrunden, då sätts pixel till värdet för bakgrunden.

			[b]Opening[/b]
			Defineras som en erosion följd av en dilation med samma strukturelement: [math]A @ B = (A &#8854; B) o+ B[/math].

			[b]Closing[/b]
			Defineras som en dilation följd av en erosion med samma strukturelement: [math]A &#8226; B = (A o+ B) &#8854; B[/math].

			[b]Hit-and-miss transform[/b]
			[math]A ox (J, K) = (A &#8854; J) cup (A^C &#8854; K)[/math]. Kan användas för att hörn i bilder.

			[b]Thinning[/b]
			[math]text(thin)(A, B) = A - text(hit-and-miss)(A, B)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad är ett receptive field?
		</Question>
		<Answer>
			En region i det visuella fältet där visuella sensorer/neuroner/operatorer reagerar emot visuell stimulans.
			<!-- Covariant receptive fields? -->
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad innebär en "scale-space" representation?
		</Question>
		<Answer>
			Det är ett sätt att representera strukturer i bilder vid olika typer av skalor.
			Givet en någon bild [math]f[/math], definiera en familj av interna representationer: [math]L(*, s) = bb "T"_s f[/math] med en parameter [math]s[/math], för en familj av operatorer [math]bb "T"_s[/math] som uppfyller:

			[b]Linjäritet[/b]
			[math]bb "T"_s(a_1 f_1 + a_2 f_2) = a_1 "T"_s f_1 + b_1 "T"_s f_2[/math]

			[b]Shift-invariance[/b]
			[math]bb "T"_s(S_(Delta x) f) = S_(Delta x)(bb "T"_s f)[/math] där [math]S_(Delta x)[/math] är shift operatorn, [math](S_(Delta X) f)(x) = f(x - Delta x)[/math].

			[b]Semi grupp struktur över [math]s[/math][/b]
			[math]bb "T"_(s_1) bb "T"_(s_2) = bb "T"_(s_1+s_2)[/math]

			[b]Skal kovarians under transformation [math]x' = S x[/math][/b]
			[math]L'(x'; s') = L(x; s)[/math], samma sak som: [math]bb "T"_(bb "S"(s)) bb "S" f = bb "S" bb "T"_s f[/math].

			[b]"Non-enhancement of local extrema"[/b]
			Om vid någon skala [math]s_0[/math] så är en punkt [math]x_0 in R^N[/math] ett lokalt maximum/minimum för avbildningen [math]x |-> L(x; s_0)[/math] så gäller:
			[ul]
				[li][math](del_s L)(x; s) le 0[/math] vid något spatialt maximum.[/li]
				[li][math](del_s L)(x; 0) >= 0[/math] vid något spatialt minimum.[/li]
			[/ul]Om allt detta gäller för en "scale-space" representation, då gäller: [math]del_s L = 1/2 nabla_x^T(Sigma_0 nabla_x L) - delta_0^T nabla_x L[/math] för någon [math]2 xx 2[/math] kovarians matris [math]Sigma_0[/math] och någon 2D vektor [math]delta_0[/math] med [math]nabla_x = (del_(x_1), del_(x_2))^T[/math].
 		</Answer>
	</TestQuestion>
	<!-- <TestQuestion Category="FeatureDetection">
		<Question>
			Vad är en gaussian receptive field?
		</Question>
		<Answer>
			Det är något som kräver att kernels är symmetriska under rotation
		</Answer>
	</TestQuestion> -->
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur definerar vi kanter i teorin, och vad för problem finns det med i praktiken?
		</Question>
		<Answer>
			I teorin så definierar vi kanter som "discontinuity" i bilden. Men i praktiken så existerar det inte, på grund av brus.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/Edges.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för problem finns det när vi deriverar en bild, och hur kan vi lösa det?
		</Question>
		<Answer>
			Derivatan är mycket känslig mot brus. En liten förändring av indata kan leda till en godtycklig stor förändring av utdata. Exempel är funktionen [math]f(x) = arctan(x)[/math] med derivatan [math]f'(x) = 1 / (1 + x^2)[/math].

			Vi kan lösa det genom att smootha bilden innan vi deriverar. Hur mycket vi ska smootha beror på olika faktorer. Om vi ökar hur mycket vi smoothar så tar vi bort bruset bättre, men det leder till större förvrängningar av den sanna strukturen. Om vi minskar hur mycket vi smoothar så får vi stället mer exakta strukturer, men större antal falska positiva.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för grundläggande metoder finns det för att hitta kanter?
		</Question>
		<Answer>
			[b]Linjära[/b]
			[ul]
				[li]Derivering.[/li]
				[li]High-pass filtrering.[/li]
				[li]Matcha med modell mönster.[/li]
			[/ul][b]Icke-linjära[/b]
			[ul]
				[li]Passa en parametriserad modell av kanter till data.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för attributer och problem finns det med kanter?
		</Question>
		<Answer>
			[b]Attributer[/b]
			[ul]
				[li]Position.[/li]
				[li]Orientering.[/li]
				[li]Stryka.[/li]
				[li]Bredd.[/li]
			[/ul][b]Problem.[/b]
			[ul]
				[li]Brus.[/li]
				[li]Störning ifrån närliggande strukturer.[/li]
				[li]Fysisk tolkning.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar en kantupptäckare baserad på Laplacian?
		</Question>
		<Answer>
			För 2D signaler så är Laplacian [math]nabla^2 L = L_(x x) + L_(y y)[/math] symmetrisk under rotation som sammanfaller med andra-ordningens derivator längs 1D raka linjer. Detta ger att vi kan försöka hitta kanter genom att kolla där Laplacian korsar origo: [math]nabla^2(g(*; t) ** f) = 0[/math].

			Det finns dock stora problem:
			[ul]
				[li]Korsningar av origo ger gäller också för "falska kanter".[/li]
				[li]Dålig lokalisering av kurvade kanter.[/li]
				[li]Detta ger en kantupptäckare som inte är särskilt bra.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar Cannys kantupptäckare?
		</Question>
		<Answer>
			[ol]
				[li]Smootha bilden.[/li]
				[li]Estimera kantens styrka och normal.[/li]
				[li]Sätt en gräns för kantens styrka, och behåll endast de kanter som är lokala extremvärden av kanternas styrka i kantens normalriktning.[/li]
			[/ol]Problemen med att sätta en gräns för kantens styrka är att detta kan ledda till fragmenterade kanter. Vi får för många lokala maximum pga. brus om gränsen är för låg, eller att kanter delar sig vid svaga punkter om gränsen är för låg. Vi kan försöka lösa detta genom att introducera två gränser [math]T_text(low)[/math] och [math]T_text(high)[/math]. I första fasen så tillåter vi endast kanter om styrken är större än [math]T_text(low)[/math], och i andra fasen så bevarar vi endast kanter om styrkan är större än [math]T_text(high)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar en kantupptäckare baserad på differentialgeometri?
		</Question>
		<Answer>
			[ol]
				[li]Smootha bilden.[/li]
				[li]Beräkna [math]L_(v v)^(text(~)), L_(v v v)^(text(~))[/math] för varje punkt.[/li]
				[li]Sök efter [math]L_(v v)^(text(~)) = 0[/math] där [math]L_(v v v)^(text(~)) lt 0[/math] gäller.[/li]
				[li]Kan sedan kombineras med en gräns baserad på magnituden av gradienten.[/li]
			[/ol][math]L_(v v)^(text(~)) = L_x^2 L_(x x) + 2L_x L_y L_(x y) + L_y^2 L_(y y)[/math]
			[math]L_(v v v)^(text(~)) = L_x^3 L_(x x x) 3 L_x^2 L_y l_(x x y) + 3 L_x L_y^2 L_(x y y) + L_y^3 L_(y y y )[/math]

			Fördelarna är att vi hanterar sammankoppling av kanter automatiskt och undviker problem med estimering av orientering av kanten.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för grundläggande krav finns det på en "interest point"?
		</Question>
		<Answer>
			[ul]
				[li]Matematiskt väldefinierad.[/li]
				[li]Har en väldefinierad position i bilden.[/li]
				[li]Vara rik i information i del av bilden så att "interest point" kan matchas lätt.[/li]
				[li]Vara stabil under naturlig transformation: vyriktning, vyavstånd, variation i ljussättning.[/li]
				[li]Vara tillräckligt annorlunda så att vi kan separera olika punkter.[/li]
				[li]Ha en skalattribut för hantera variationer i skala.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för olika metoder finns det för att hitta "interest points"?
		</Question>
		<Answer>
			[ul]
				[li]Harris corner detection[/li]
				[li]Laplacian blob detection with automatic scale selection. Approximeras av SIFT.[/li]
				[li]Determinant av Hessian blob detection with automatic scale selection. Approximeras av SUFT.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad innehåller moment matrisen av andra ordningen?
		</Question>
		<Answer>
			Ackumulerad statistisk om lokala riktningar inom ett område runt varje punkt. [math]mu(x; t, s) = int_(R^2) (nabla L(*; t)) (nabla L(*; t))^T g(x - q; s) dx[/math] där [math]t[/math] är en lokal skala för att beräkna derivator, [math]s[/math] en skala för att beräkna statistisk av gradient riktningar och [math]g(*; s)[/math] en fönsterfunktion vilket statistiken ackumuleras över.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur definerars Harris operatorn?
		</Question>
		<Answer>
			Givet moment matrisen av andra ordningen [math]mu[/math] med egenvärdena [math]lambda_1, lambda_2 >= 0[/math] så beräknar vi Harris måttet [math]H = det mu - k * trace(mu)^2 = lambda_1 lambda_2 - k(lambda_1 + lambda_2)^2[/math] där [math]k in [0, 1/4][/math]. Ett högre värde för [math]k[/math] ger en mer restriktiv detektor.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för egenskaper av moment matrisen av andra ordningen?
		</Question>
		<Answer>
			[ul]
				[li]Smooth region i bilden utan starka kantstrukturer: [math]lambda_1, lambda_2[/math] små.[/li]
				[li]Längs en rak kant: [math]lambda_1 > lambda_2[/math].[/li]
				[li]Vid ett hörn med två eller flera dominanta riktningar: [math]lambda_1, lambda_2[/math] stora.[/li]
				[li]För att [math]H[/math] ska reagera så måste båda egenvärdena vara stora.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar Harris corner detector?
		</Question>
		<Answer>
			[ol]
				[li]Beräkna [math]L_x, L_y[/math] vid någon skala [math]t[/math][/li]
				[li]Beräkna [math]L_x^2, L_x L_y, L_y^2[/math] för varje pixel.[/li]
				[li]Beräkna [math]mu = ((E(L_x^2), E(L_x L_y)), (E(L_x L_y), E(L_y^2)))[/math] för någon fönsterfunktion [math]E[/math] vid någon skala [math]s[/math][/li]
				[li]Beräkna [math]H = E(L_x^2) E(L_y^2) - (E(L_x L_y)^2 - k (E(L_x^2 + E(L_y^2))))^2[/math] för varje pixel.[/li]
				[li]Hitta lokala extemvärden av [math]H[/math] som uppfyller [math]H >= H_0 > 0[/math].[/li]
			[/ol]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar Laplacian blob detector?
		</Question>
		<Answer>
			Givet en "scale-space" representation av [math]L[/math] av en bild [math]f[/math] som vi har fått av gaussian smoothing: [math]L(*; t) = g(*; t) ** f[/math].

			Då beräknar vi Laplacian: [math]nabla^2 L = L_(x x) + L_(y y)[/math]. Spatiala extremvärden av [math]nabla^2 L[/math] anses då vara en blob med:
			[ul]
				[li][math]nabla^2 L lt 0[/math]: ljus blob.[/li]
				[li][math]nabla^2 L > 0[/math]: mörk blob.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad är Hessian matrisen?
		</Question>
		<Answer>
			Det är en kvadratisk matris som innehåller en funktions alla partiella andraderivator. Dvs: Låt [math]f(x_1, x_2, ..., x_n)[/math] vara en funktion med existerande andraderivator. Då är hessians matrisen [math]H(f)=[[(del^2 f) / (del x_1 x_1), (del^2 f) / (del x_1 x_2), ..., (del^2 f) / (del x_1 x_n)], [(del^2 f) / (del x_2 x_1), (del^2 f) / (del x_2 x_2), ..., (del^2 f) / (del x_2 x_n)], [vdots, vdots, ddots, vdots], [(del^2 f) / (del x_n x_1), (del^2 f) / (del x_n x_2), ..., (del^2 f) / (del x_n x_n)]][/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar Determinant av Hessian blob detection?
		</Question>
		<Answer>
			Givet en "scale-space" representation av [math]L[/math] av en bild [math]f[/math] som vi har fått av gaussian smoothing: [math]L(*; t) = g(*; t) ** f[/math].

			Då beräknar vi determinaten av Hessian matrisen: [math]det(H(L))=L_(x x) L_(y y) - L_(x y)^2[/math]. Lokala extremvärden av [math]det(H(L))[/math] anses då vara en blob med (tror dock detta är fel, bör nog vara [math]det(H(L))[/math]):
			[ul]
				[li][math]nabla^2 L lt 0[/math]: ljus blob.[/li]
				[li][math]nabla^2 L > 0[/math]: mörk blob.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad är skillnanden av "interest points" beräknad av Laplacian och determinaten av Hessian matrisen?
		</Question>
		<Answer>
			I en referensram som är justerad till egenvärdena av Hessian matrisen, [math]H(L) = [[lambda_1, 0], [0, lambda_2]][/math] så ges operatorerna av: [math]nabla^2 L = lambda_1 + lambda_2[/math], [math]det(H(L)) = lambda_1 lambda_2[/math]

			Då gäller:
			[ul]
				[li]För att Laplacian ska ge en stark respons så krävs det endast att det är en stark respons i någon av koordinat riktningarna ([math]lambda_1[/math] eller [math]lambda_2[/math]). Detta ger risk för felaktig respons nära kanter, och kan undvikas med sätta en gräns.[/li]
				[li]För determinanten av Hessian matrisen ska ge en stark respons så krävs det att en stark respoins i båda koordinat riktningarna.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar algoritmen för att extremvärden i "scale-space"?
		</Question>
		<Answer>
			[ol]
				[li]Givet ett intervall av skalor [math][t_text(min), t_text(max)][/math], fördela mängden av skalor likformigt i termer av effektiv skala: [math]tau = log t[/math][/li]
				[li]Applicera gaussian smoothing till bilden vid varje skalnivå.[/li]
				[li]För varje pixel i bilden, beräkna derivator och kombinera dessa så att vi får någon invarians.[/li]
				[li]Hitta lokala extremvärden i både skala och rummet genom att jämföra med närmaste grannar i en [math]3 xx 3 xx 3[/math] region, med en gräns på magnituden av responsen.[/li]
				[li]Sortera "interest points" i sjunkande ordning baserat på skalnormaliserade magnitud värden.[/li]
			[/ol]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad är en pyramid?
		</Question>
		<Answer>
			När man ökar skalan, så blir strukturer av små storlek dämpade. Man borde då kunna minska storleken av bilden (subsampla) för att göra algoritmen mer effektiv. Detta är idén bakom pyramider, att man kombinerar smoothing med subsampling för att öka effektiveten.

			Det finns också hybridpyramider, där man varierar när man subsamplar för att avväga effektivitet och precision.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar SIFT deskriptorn?
		</Question>
		<Answer>
			<!-- [ol]
				[li]Beräkna gradienten [math]nabla L[/math] runt [math](x_0, y_0)[/math] vid en skala [math]hat(t)[/math] runt en interest point.[/li]
				[li]Skapa ett histogram av orienteringar, genom att beräkna riktningen för den lokala gradienten över 36 bins. De bins med största värden är en approximation av orienteringen.[/li]
				[li]Definera ett [math]4 xx 4[/math] rutnät runt interest point. Ackumulera histogramet av riktningar för gradient riktningar till 8 bins. [/li]
				[li][/li]
			[/ol] -->
			Idén är att upptäcka features som skillnader i gaussian, och använda flera skalor för att bli invariant emot skalor.

			[ul]
				[li]Deskriptorn består av histogram av lokala gradient riktningar.[/li]
				[li]Dominerande riktning som referens. Detta ger invarians under rotation.[/li]
				[li]Fönsterstorlek från scale-space detektor. Detta ger skalinvarians[/li]
				[li]Normalisera feature-vektorn så att längden blir 1. Detta ger invarians emot förändring av ljus.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur fungerar SUFT deskriptorn?
		</Question>
		<Answer>
			Verkar mycket komplicerat, kolla senare.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Hur kan man matcha deskriptorer mellan två bilder?
		</Question>
		<Answer>
			[ul]
				[li]Givet två bilder [math]f_A[/math] och [math]f_B[/math], beräkna interest points för båda bilder: [math]A = { A_i }, B = { B_i }[/math].[/li]
				[li]Jämför deskriptorerna genom att beräkna euklidiska avståndet mellan de deskriptorerna för båda bilderna.[/li]
				[li]Acceptera en matchning mellan ett par av interest points [math](A_i, B_j)[/math] om:
					[ul]
						[li][math]A_i[/math] är den bästa matchningen för [math]B_j[/math] i relation till alla andra punkter i [math]A[/math][/li]
						[li][math]B_j[/math] är den bästa matchningen för [math]A_i[/math] i relation till alla andra punkter i [math]B[/math][/li]
					[/ul]
				[/li]
				[li]För att förhindra tvetydliga matchningar, lägg till ett krav att kvoten mellan avståndet mellan de två närmaste deskriptorerna vara mindre än 0.9.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="FeatureDetection">
		<Question>
			Vad för slutsatser kan vi dra om olika interest point detectors?
		</Question>
		<Answer>
			[ul]
				[li][math]det(H(L))[/math] är en bättre interest point detector än [math]nabla^2 L[/math].[/li]
				[li]SIFT är en bättre deskriptor än SURF, men SURF kan beräknas snabbare.[/li]
				[li]Både [math]det(H(L))[/math] och [math]nabla^2 L[/math] är mycket bättre interest point detectors än Harris-Laplace.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Vad är skillnaden mellan igenkänning (recognition) och klassificering (classification)
		</Question>
		<Answer>
			[ul]
				[li]Igenkänning: Är det här mitt objekt?[/li]
				[li]Klassificering: Vad för objekt är det här?[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Ange skillnader mellan olika metoder för objektigenkänning.
		</Question>
		<Answer>
			[ul]
				[li]Supervised vs unsupervised
					[ul]
						[li]Supervised: annoterat data finns.[/li]
						[li]Unsupervised (clustering): inga annoterade data finns.[/li]
					[/ul]
				[/li]
				[li]Single-class vs multi-class
					[ul]
						[li]Single-class: Är det här en kopp?[/li]
						[li]Multi-class: Är det här en kopp, bil, person, etc. ?[/li]
					[/ul]
				[/li]
				[li]
					Generative vs discriminative
					[ul]
						[li]Generative: modellerar hur saker ser ut.[/li]
						[li]Discriminative: modellerar skillnader mellan saker.[/li]
					[/ul]
				[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Vad bör en metod för objektigenkänning vara invariant emot?
		</Question>
		<Answer>
			[ul]
				[li]Förändring i ljussättning (skuggning, färg).[/li]
				[li]Position/orientering (vyriktning).[/li]
				[li]Deformation (förändringar i form).[/li]
				[li]Skymning av objektet.[/li]
				[li]Storlek i bilden (dvs vyavstånd).[/li]
				[li]Kameran (projektioner och förvrängningar).[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Ange olika metoder för objektigenkänning.
		</Question>
		<Answer>
			[ul]
				[li]Template based (dense)
					[ul]
						[li]Spara en bild av objektet.[/li]
						[li]Relativ position mycket viktigt.[/li]
						[li]Normalisera för invarians emot ljussättning och skala.[/li]
					[/ul]
				[/li]
				[li]Feature based (sparse)
					[ul]
						[li]Spara en mängd av features (hörn, konturer).[/li]
						[li]Relativ position är flexibel.[/li]
						[li]Behåller endast information som kan vara invariant.[/li]
					[/ul]
				[/li]
				[li]Statistics based (usally dense)
					[ul]
						[li]Spara histogram av bilddata (kanter, färger).[/li]
						[li]Position ignoreras.[/li]
						[li]Olika typer av data matchas invariant.[/li]
					[/ul]
				[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Vad innebär Bag of Words?
		</Question>
		<Answer>
			Det är ett sätt för att hantera många (miljoner) bilder. Idén är att representera bilden som ett histogram av features (oftast SIFT). Det fungerar på följande sätt:
			[ol]
				[li]Extrahera features (t.ex. SIFT).[/li]
				[li]Klustra dessa features (t.ex. K-means).[/li]
				[li]Beräkna histogram av features.[/li]
				[li]Klassificera histogram till olika klasser av objekt.[/li]
			[/ol]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Hur fungerar en nearest neighbor klassificerare?
		</Question>
		<Answer>
			Givet en test bild, hitta den närmaste grannen bland alla träningsexempel. Problemet är antalet träningsexemplar kan vara många och söka efter närmaste grannen kan vara kostsamt.

			[b]Fördelar[/b]
			[ul]
				[li]Fungerar bra för klasser som kan separeras väl.[/li]
				[li]Kan representera kluster med komplex form.[/li]
			[/ul][b]Nackdelar[/b]
			[ul]
				[li]Kan kräva komplexa beräkningar i högre dimensioner.[/li]
				[li]Beror mycket på hur vi bestämmer "närmaste granne".[/li]
				[li]Mycket känslig till utstickare (outlier).[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="ObjectRecognition">
		<Question>
			Hur fungerar Bayesian klassificering?
		</Question>
		<Answer>
			Vi vill välja den klass [math]k[/math] som maximerar [math]p(k | z) = (p(z | k) p(k)) / (p(z))[/math]. Oftast gör vi ett maximum likelihood antagande, att [math]p(k)[/math] är samma för alla klasser. För att hitta [math]p(z | k)[/math] så kan olika algoritmer användas, t.ex. SVM eller Logistic regression. Detta var det som mönsterigenkännings kursen handla om, så kolla där.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad innebär stero?
		</Question>
		<Answer>
			Det innebär att man beräknar djupet av ett objekt genom att jämföra skillnader i objekts position mellan två eller flera kameror.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är "cyclopean eye"?
		</Question>
		<Answer>
			Det är en modell för att beskriva hur synen fungerar. Den säger att vår syn egentligen är ett öga, som ligger halvvägs mellan båda ögonen.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är "fixation point"?
		</Question>
		<Answer>
			Det är punkten där två kameror konvergerar emot. Riktningen som kameran pekar emot kallas för principal direction.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är "vergence"?
		</Question>
		<Answer>
			Det är vinkeln mellan principal direction mellan två kameror, och betecknas [math]2 mu[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är "gaze"?
		</Question>
		<Answer>
			Det är vinkeln mellan huvudriktningen och riktningen ifrån cyclopean eye till fixation point, betecknas [math]gamma[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Ange relationen mellan vergence, gaze och cyclopean eye.
		</Question>
		<Answer>
			Vi har [math]mu = (rho_L - rho_R) / 2, gamma = (rho_L + rho_R) / 2[/math]. Beskrivs av bilden nedan:
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/SteroGeometry.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är parallax?
		</Question>
		<Answer>
			Skenbara rörelsen av 3D objekt som är placerade på olika avstånd. Bör nog kolla på wikipedia efter bättre förklaring.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Hur lyder geometrin för parallella kameror?
		</Question>
		<Answer>
			Låt den vänstra kameran vara på position [math](0, 0, 0)[/math] och den högra på [math](T_x, 0, 0)[/math]. Utöver det, så är båda kamerorna identiska. Antag att en punkt [math](X, Y, Z)[/math] har koordinaterna [math]x_l = f X / Z, x_r = f Y / Z[/math] i den vänstra kameran. Då är dess koordinater i den högra kameran: [math]x_r = f (X - T_x) / Z, y_r = f Y / Z[/math].

			Då kan vi beräkna disparity som: [math]d = x_l - x_r = f X / Z - (f X / Z - f T_x / Z)) = f T_x / Z[/math]. Detta innebär: [math]Z = (f T_x) / d[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är disparity?
		</Question>
		<Answer>
			Låt [math](x_L, y_L)[/math] och [math](x_R, y_R)[/math] vara samma punkt i olika kameror. Då kallas skillnanden för [i]disparity[/i]. Vi har horisontell disparity: [math]x_R - x_L[/math] och vertikal: [math]y_R - y_L[/math]. Disparity betecknas som [math]d[/math]. Om vi låter [math]b[/math] bara baselinen (förflyttningen mellan kamerorna), då gäller: [math]Z = (bf) / d[/math]. Detta innebär att vi kan beräkna djupet som skillnaden i position mellan kamerorna.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad för värde bör vi ha på baselinen?
		</Question>
		<Answer>
			Om vi deriverar [math]Z = (bf) / d[/math] med anseende på [math]d[/math] får vi: [math](del Z) / (del d) = -(Z^2)/(bf)[/math]. Detta innebär att felet i djup beror kvadratiskt på djupet. Detta ger även om vi ökar baselinen, så får vi ett mindre fel. Dock blir problemet att matcha punkter mycket svårare, på grund av större perspektiv förvrängningar.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är en "epipolar plane"?
		</Question>
		<Answer>
			Det är ett plan genom en punkt [math]X[/math] och kamerornas optiska centrum.
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/EpipolarPlane.png[/img].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är en "epipole"?
		</Question>
		<Answer>
			Det är projektionen av de optiska centret, och betecknas [math]p(p')[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är en "epipolar line"?
		</Question>
		<Answer>
			Det är en linje som projiceras till ett epipolar plane som går genom ett optiskt center.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är en "epipolar transform"?
		</Question>
		<Answer>
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/EpipolarTransform.png[/img]

			Det är en avbildning från punkter i bildplanet till epipolar linjer i den andra bildplanet.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är en "essential matrix"?
		</Question>
		<Answer>
			Antag att två kameror är relaterade med [math]R[/math] en rotation och [math]t[/math] en transformation. Då kallas matrisen [math]E = R T_t, T_t = ((0, t_3, -t_2), (-t_3, 0, t_1), (t_2, -t_1, 0))[/math] för "essential matrix". Motsvarande puntker [math]q, q'[/math] är då relaterade av [math](q')^T E q = 0[/math]. Der går även att se det som att essential matrisen villkor för epipolar linjer.

			Linjen [math]I_q = Eq[/math] är epipolar linjen som motsvarar [math]q[/math] under en epipolar transformation, och [math](q')^T I_q = 0[/math] för dden andra bilden.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Hur kan man beräkna "essential matrix"?
		</Question>
		<Answer>
			Först, använd features (t.ex. SIFT) för att matchning mellan två bilder. Vi kan sedan ställa upp ett ekvationssystem med 6 okända (tre för rotation och tre för förflyttning). Vi kan dock ej bestämma magnituden för förflyttningen, vilket ger att vi endast får 5 okända. Detta innebär att vi behöver 5 matchningar.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad för problem finns det med att beräkna matchningar mellan bilder?
		</Question>
		<Answer>
			[ul]
				[li]Variation i ljussättning.[/li]
				[li]Förvrängningar pga. perspektiv.[/li]
				[li]Upprepade mönster.[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Hur kan man använda sig av korrelation för att beräkna matchningar?
		</Question>
		<Answer>
			Vi har ett fönster, som vi förflyttar över båda bilderna. Vi beräknar sedan en korrelation, för att se hur bra fönsterna matchar. Man kan antigen använda sig av [math]text(SSD) = sum_([i, j] in R) (f(i, j) - g(i, j))^2[/math] eller [math]text(SAD) = sum_([i, j] in R) ||f(i, j) - g(i, j)||[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="StereoGeometry">
		<Question>
			Vad är "Disparity Space Image"?
		</Question>
		<Answer>
			Hitta svar senare!
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Vad för olika typer av rörelser finns det i bilder?
		</Question>
		<Answer>
			[ul]
				[li]Rörelse av kamera (ego-motion).[/li]
				[li]Rörelse av saker i bilden (independent motion).[/li]
			[/ul]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Hur förändras en punkt pga. av rörelsen av kameran (även betecknat motion field)?
		</Question>
		<Answer>
			Antag att kameran rör sig med vinkelhastighet [math]omega[/math] och linjärhastighet [math]T[/math]. En punkt [math]P=(X, Y, Z)[/math] rör sig då (jämförelse med kameran) enligt: [math]dot(P) = -T - omega xx P[/math].

			Rörelsen i bilden sker enligt: [math]((dot x), (dot y)) = f / Z ((-T_x + x/f T_z), (-T_y + y / f T_z)) + ((omega_x (xy)/f - omega_y (f + (x^2)/f) + omega_z y), (omega_x (f + (y^2)/f) - omega_y (xy) / f + omega_z x))[/math]. Den första komponenten relaterar till förflyttning medan den andra relaterar till rotationen. Slutsatsen av denna formel är att skalan är tvetydlig och rotationen är oberoende av djupet, vilket ger att det inte går att estimera djupet genom att endast rotera - vi måste förflytta kameran också
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Vad är "focus of expansion"?
		</Question>
		<Answer>
			Det är punkten som ett fält expanderar ifrån. Vi har: [math]((x_text(FOE)), (y_text(FOE)), (f)) = f / T_z ((T_x), (T_y), (T_z))[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Vad är optical flow?
		</Question>
		<Answer>
			Det är den skenbara rörelsen av "brightness patterns" (vad är ett bra svenskt ord?). Oftast så är optical flow samma sak som motion field, men inte alltid. Se bild nedan.

			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/OpticalFlowAndMotionField.png[/img]
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Vad är "optical flow constraint equation"?
		</Question>
		<Answer>
			Det är en ekvation som ger begräsningar på komponenterna för optical flow. Antag att [math]vec(u) = ((u, v))[/math] är optical flow vektorn, och [math]I_x, I_y, I_t[/math] derivatan av [math]I(x(t), y(t), t)[/math] med hänseende på [math]x, y, t[/math].

			Om vi antar att intensiteten för varje synlig punkt i scenen är konstant över tiden: [math](d I) / (d t) = 0[/math], så får vi [math]I_x u + I_y v + I_t = 0[/math] (optical flow constraint equation).

			Den går också att skriva som: [math]((I_x, I_y)) * ((u, v)) = - I_t rArr ((u, v)) = -I_t / (I_x^2 + I_y ^2) ((I_x, I_y))[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Vad är "aperture problem"?
		</Question>
		<Answer>
			Att vi inte kan bestämma optical flow längs en kant.
		</Answer>
	</TestQuestion>
	<TestQuestion Category="MotionAndOpticalFlow">
		<Question>
			Hur kan vi bestämma optical flow?
		</Question>
		<Answer>
			Vi ställer ett överbestämt ekvationssystem och löser med minstakvadratmetoden.
			[math][[I_(x_1), I_(y_1)], [I_(x_2), I_(y_2)], [vdots, vdots]] [[u], [v]] = -[[I_(t_1)], [I_(t_2)], [vdots]] hArr A vec(u) = b[/math].

			Lösningen blir då [math]vec(u) = (A^T A)^-1 A^T b[/math]. Vi får att [math]A^T A = [[sum I_x^2, sum I_x I_y], [sum I_x I_y, sum I_y^2]][/math], vilket är samma sak som second moment matrisen, vilket används för att hitta hörn.

			Kravet för att denna matris ska vara inverterbar är att det inte finns några egenvärden som är noll. När det är en kant så får vi det, pga. [math][[-I_y, I_x]][/math] är ett egenvektor med egenvärdet 0.

			Det finns ytterligare beteende pga. det är second moment matrisen som vi använder oss av:
			[img width="75%" height="75%"]https://dl.dropboxusercontent.com/u/4940720/TentaPluggGenerator/images/OpticalFlowSecondMoment.png[/img]
		</Answer>
	</TestQuestion>
</Test>
