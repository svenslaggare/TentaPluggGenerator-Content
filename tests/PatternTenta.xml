<?xml version="1.0" encoding="utf-8" ?>
<Test Name="Tenta">
	<TestQuestion Category="BayesianPatternClassification">
		<Question>
				Vad är definitionen av Bayesian Pattern Classification?
		</Question>
		<Answer>
			[ul]
				[li]En signalkälla som slumpmässigt väljer en intern källkategori [math]S in {1,...,N_s}[/math][/li]
				[li]Den genererade signalen kan sedan transformeras och brus kan tillkomma.[/li]
				[li]Källkategorin är okänd för den externa världen, det ändå som klassificeraren ser är en feature vektor [math]x[/math] som består av [math]K[/math] olika features.[/li]
				[li]Beroende på de observerade features, så väljer klassificeraren en utgärd ifrån en lista av [math]N_d[/math] olika alternativ.[/li]
				[li]Utvalet baseras på [math]N_d[/math] olika discriminant funktioner, där den funktionen som högst värde väljs.[/li]
				[li]Utdata är ett index [math]d in {1,...,N_d}[/math].[/li]
			[/ul]
			[b]Prior källkategori distribution[/b]
			Källkategorin betraktas som en diskret slumpvariabel [math]S[/math] vilket tar värdet [math]S=j[/math] med sannolikhet [math]P_S(j)[/math], denna fördelning kallas för [i]a priori probability distribution[/i].

			[b]Feature vektor distribution[/b]
			Den observerade feature vektorn [math]x[/math] betraktas som ett utfall ifrån slumpvariabeln [math]X[/math], som [i]måste[/i] bero på källkategorin. Denna betecknas som: [math]f_(X|S)(x | j)[/math].

			[b]Beslutsfunktion[/b]
			Mappar ifrån observerad feature vektor [math]x[/math] till ett beslut, [math]d(x) in {1,..., N_d}[/math].

			[b]A Posteriori Source category Distribution[/b]
			Ges av Bayes regel: [math]P_(S|X)(j | x) = (f_(X|S)(x | j) P_S(j)) / f_X(x)[/math] där [math]f_X(x) = sum_(j=1)^(N_s) f_(X|S)(x | j) P_S(j)[/math].
		</Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianPatternClassification">
	    <Question>
			Vad innebär Bayes Minimum-Risk Decision Rule?
	    </Question>
	    <Answer>
			Först måste vi definiera ett generellt optimerings kriterium. Definiera en [i]loss matris[/i] av storlek [math]N_d \times N_s[/math]:
			[math]L = [[L(d = 1 | S = 1), ..., L(d = 1 | S = N_s)], [..., ..., ...], [L(d = N_d | S = 1), ..., L(d = N_d | S = N_s)]][/math].
			Där [math]L_(i, j) = L(d = i | S = j)[/math] ger oss förlusten som vi får om vi väljer [math]d = i[/math] när den faktiska kategorin är [math]S = j[/math].

			Vi introducerar nu: Conditional Expected Loss: [math]R(i | x) = sum_(j = 1)^(N_s) L(d = i |S = j) P_(S|X)(j | x)[/math]. Regeln blir då: [math]d(x) = argmin_i R(i | x)[/math].
			<!-- Något om varför är fallet, Q grejen i boken. -->
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianPatternClassification">
	    <Question>
			Vad innebär Minimum Error Rate?
	    </Question>
	    <Answer>
			Det är ett special fall av Bayes Minimum-Risk Decision Rule. Om syftet är att gissa den gömda källkategori med minska sannolikhet för fel, då kan vi definiera loss matrisen som: [math]L(d = i | S = j) = { (0, i = j), (1, text{annars}) :}[/math]. Detta innebär att rätta svar kostar oss ingenting, och alla fel svar kostar lika mycket. Detta ger: [math]R(i | x) = sum_(j = 1)^(N_s) L(d = i | S = j) P_(S|X)(j | x) = sum_(j != i) P_(S|X)(j|x) = 1 - P_(S|X)(i | x)[/math].

			För att minimera risken, så bör klassificeraren välja den källa som hade högst värde för posteriori conditional probability, [math]P_(S|X)(i | x)[/math]. [math]d(x) = argmax_i P_(S|X)(i | x) \hArr argmax_i (f_(X|S)(x|i)P_S(i)) / f_X(x)[/math].

			För att [math]f_X(x)[/math] är lika för alla källor, så kan ekvationen förenklas till: [math] d(x) = argmax_i f_(X|S)(x | i) P_S(i)[/math], som kallas för [i]Maximum a Posteriori probability (MAP) Decision Rule[/i]. 
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianPatternClassification">
	    <Question>
	    	Vad innebär Maximum Likelihood (ML) Decision Rule?
	    </Question>
	    <Answer>
	    	Om alla källkategorier är lika sannolika, då kan man förenkla MAP regeln till ML: [math]d(x) = argmax_i f_(X|S) (x | i)[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianPatternClassification">
	    <Question>
	    	Vad innebär en discriminant funktion?
	    </Question>
	    <Answer>
	    	Alla tidigare beslutsregler kan uttryckas i en generell form (General Decision Rule): [math]d(x) = argmax_(i = 1, ..., N_d) g_i(x)[/math] där [math]g(x)[/math] funktionerna kallas för discriminant funktioner. Då kan man få tidigare regler som:
	    	[ul]
	    		[li]Min. Risk Rule: [math]g_i(x) = -R(i | x)[/math][/li]
	    		[li]MAP Rule: [math]g_i(x) = f_(X|S)(x | i) P_S(i)[/math][/li]
	    		[li]ML Rule: [math]g_i(x) = f_X(X|S)(x | i)[/math][/li]
	    	[/ul]Vi kan få en ny mängd av discriminant funktioner, med identiska beslut om vi applicerar transformationen [math]g^'_i(x) = h(g_i(x))[/math] där [math]h[/math] är en monotomisk ökande funktion (t.ex. [math]log[/math]).
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianPatternClassification">
	    <Question>
	    	Vad innebär en decision region?
	    </Question>
	    <Answer>
	    	För varje observerad feature vektor [math]x[/math] så definierar [math]d(x)[/math] en unik utdata för klassificeraren. Vi kan se denna funktion som delar upp mängden av alla feature vectors till disjunkta decision regioner: [math]Omega_i = { x: d(x) = i }[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianPatternClassification">
	    <Question>
	    	Hur kan vi beräkna sannolikhet för fel?
	    </Question>
	    <Answer>
	    	Om vi bara har två källor: [math]P[text{error}] = P[X in Omega_2 \cap S = 1] + P[X in Omega_1 \cap S = 2][/math]. Detta är lika med: [math]P_S(1) int_(Omega_2) f_(X|S)(x | 1) dx + P_S(2) int_(Omega_1) f_(X|S)(x | 2) dx[/math].

	    	I det mer generella fallet då [math]N_d = N_s > 2[/math] så blir det mer komplicerat, för det finns fler sätt att vara fel på än rätt. Då är det lättare att istället beräkna [math]P[text{correct}]=sum_(i=1)^(N_s) P[X in Omega_i \cap S = i] = sum_(i=1)^(N_s) P_S(i) int_(Omega_i) f(X|S)(x | i) dx[/math]. Då blir [math]P[text{error}] = 1 - P[text{correct}][/math].
	    </Answer>
	</TestQuestion>
	<!-- Kansle 3.5, 3.6, 3.7, 3.8 -->
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM har infinite duration?
	    </Question>
	    <Answer>
	    	Det innebär att den "interna" sekvensen av tillstånd [i]aldrig[/i] tar slut.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM har finite duration?
	    </Question>
	    <Answer>
	    	Det innebär att den "interna" sekvensen av tillstånd är ändlig, dvs. den tar slut. Detta modelleras genom att vi introducerar ett speciellt "exit" tillstånd (betecknat [math]S_(N+1)[/math]), vilket när detta nås så avslutas sekvensen av tillstånd.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM är left-right?
	    </Question>
	    <Answer>
	    	Det innebär att endast tillåtna övergångarna för ett tillstånd är ifrån sig själv, eller ifrån ett tillstånd till "vänster" om sig själv. Detta ger att övergångsmatrisen blir "upper triangular".

	    	Om HMM är oändlig, så måste den tillslut konvergera emot det tillståndet som är mest "till höger", och är då kvar där i all evighet. Detta tillstånd kallas för "final" eller "absorbing".
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM är stationary?
	    </Question>
	    <Answer>
	    	Det innebär att sannolikheten för ett visst tillstånd vid en given tidpunkt ej ändras, dvs. är konstant över tid, alltså: [math]P[S_1 = j] = P[S_2 = j] = ... = P[S_T = j][/math] för alla [math]j[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Hur kan vi avgöra om en HMM är stationary?
	    </Question>
	    <Answer>
	    	Om och endast om: [math]p = A^T p[/math] gäller (alt: [math]p^T = p^T A[/math]). Detta ger att vi kan beräkna [math]p[/math] genom att hitta egenvektorn till [math]A^T[/math] med egenvärdet 1.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM är ergodic?
	    </Question>
	    <Answer>
	    	Om sannolikheten för att komma ifrån tillstånd [math]j[/math] till vilket tillstånd som helst [math]k[/math] med [math]n[/math] steg konvergerar emot samma värde som är större än 0, för varje tillstånd [math]j[/math]. Dvs: [math]lim_(n \rightarrow \infty) P[S_(t+n) = k | S_t = j] = u_k > 0[/math] oberoende av [math]j[/math].

	    	Detta innebär att [math]A^n[/math] måste konvergera emot en matris där alla rader är samma och större än 0, [math]lim_(n \rightarrow \infty) [A^n]_(j, k) = u_k > 0[/math], oberoende av [math]j[/math]. Vektorn [math]u[/math] är definerad som [math][[u_1], [vdots], [u_N]][/math].

	    	<!-- Kanske någon med intuitiv förklaring... -->
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Antag att en HMM är ergodic, hur kan man då beräkna den stationära sannolikhetsvektorn?
	    </Question>
	    <Answer>
	    	Det finns endast en sådan vektor, och där den är lika med vektorn [math]u[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM är irreducible?
	    </Question>
	    <Answer>
	    	Om det är möjligt att gå mellan två godtyckliga tillstånd med sannolikhet större än noll med ett ändligt antal steg.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad är en period för en HMM?
	    </Question>
	    <Answer>
	    	Ett tillstånd har en period om det är möjligt att komma tillbaka till samma tillstånd med sannolikhet större än noll med en stig som är en multipel av [math]d[/math], där [math]d[/math] är den största heltalet med denna egenskap.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad innebär det att en HMM aperiodic?
	    </Question>
	    <Answer>
	    	Om alla dess tillstånd har perioden [math]d = 1[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad gäller om en HMM är irreducible och aperiodic?
	    </Question>
	    <Answer>
	    	Att den också är ergodic.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad beräknar forward algoritmen?
	    </Question>
	    <Answer>
	    	[b]Sannolikheten för ett tillstånd givet observationerna[/b]
	    	[math]P[S_t = j | X_1 = x_1, ..., X_t = x_t, lambda] = hat alpha_(j, t)[/math]

	    	[b]Sannolikheten för nästa observation givet tidigare observationer[/b]
	    	[math]P[x_t | x_1, ..., x_(t-1), lambda] = c_t, 1 le t le T[/math]

	    	[b]Sannolikheten för avslut (för finite HMM)[/b]
	    	[math]P[S_(T + 1) = N + 1 | x_1, ..., x_T, lambda] = c_(T+1)[/math]

	    	[b]Sannolikheten för en sekvens av observationer[/b]
	    	[math]P[x_1, ..., x_t | lambda] = c_1 cdots c_t[/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Hur fungerar forward algoritmen?
	    </Question>
	    <Answer>
	    	Först, låt oss introducera några variabler.
	    	[b]Forward Variable[/b]: [math]alpha_(j, t) = P[X_1 = x_1,  ...,, X_t = x_t, S_t = j |lambda][/math].
	    	[b]Scaled Forward Variable[/b]: [math]hat alpha_(j, t) = P[S_t = j | X_1 = x_1, ..., X_t = x_t, lambda][/math].
	    	[b]Forward Scale Factors[/b]: [math]c_t = P[x_t | x_1, ..., x_(t-1), lambda][/math] för [math]1 le t le T[/math] och [math]c_(T+1) = P[S_(T + 1) = N+1 | x_1, ..., x_T, lambda][/math] för ändliga HMM.
	    	[b]Temporary Forward Variable[/b]: [math]alpha_(j, t)^(text{temp}) = P[X_t = x_t, S_t = j | x_1, ..., x_(t-1), lambda][/math].

	    	[b]Initialisering ([math]t = 1[/math])[/b]
	    	[math]alpha_(j, 1)^(text{temp}) = P[X_1 = x_1, S_1 = j | lambda] = q_j b_j(x_1)[/math] för [math]j = 1,...,N[/math]
	    	[math]c_1 = sum_(k=1)^N alpha_(k, 1)^(text{temp})[/math]
	    	[math]hat alpha_(j, 1) = alpha_(j, 1)^(text{temp}) / c_1[/math] för [math]j = 1,...,N[/math]

			[b]Steg ([math]t = 2, ..., T[/math])[/b]
			[math]alpha_(j, t)^(text{temp}) = b_j(x_t) (sum_(i=1)^N hat alpha_(i, t-1) a_(i, j))[/math] för [math]j = 1,...,N[/math]
	    	[math]c_t = sum_(k=1)^N alpha_(k, t)^(text{temp})[/math]
	    	[math]hat alpha_(j, t) = alpha_(j, t)^(text{temp}) / c_t[/math] för [math]j = 1,...,N[/math]

	    	[b]Terminering (för ändliga)[/b]
	    	[math]c_(T+1) = sum_(k=1)^N hat alpha_(k, T) a_(k, N+1)[/math]

	    	Detta ger att:
	    	Oändlig: [math]ln P[x_1, ..., x_T | lambda] = sum_(t=1)^T ln c_t[/math].
	    	Ändlig: [math]ln P[x_1, ..., x_T, S_(T+1) = N+1 | lambda] = sum_(t=1)^(T+1) ln c_t[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad beräknar backward algoritmen?
	    </Question>
	    <Answer>
	    	Sannolikheten för senare observationer givet den nuvarande tillståndet: [math]P[X_(t+1) = x_(t + 1), ..., X_T = x_t | S_t = i, lambda] = beta_(i, t)[/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Hur fungerar backward algoritmen?
	    </Question>
	    <Answer>
	    	Först, låt oss introducera några variabler:
	    	[b]Backward Variable[/b]
	    	Oändlig: [math]beta_(j, t) = P[X_(t+1) = x_(t+1), ..., X_T = x_t | S_t = i, lambda][/math].
	    	Ändlig: [math]beta_(j, t) = P[X_(t+1) = x_(t+1), ..., X_T = x_t, S_(T+1) = N+1 | S_t = i, lambda][/math].
	    	[b]Scaled Backward Variable[/b]
	    	Oändlig: [math]hat beta_(j, t) = beta_(j, t) / (c_t cdots c_T)[/math]
	    	Ändlig: [math]hat beta_(j, t) = beta_(j, t) / (c_t cdots c_T c_(T+1))[/math]

	    	[b]Initialisering ([math]t = T[/math])[/b]
	    	Oändlig: [math]beta_(i, T) = 1, hat beta_(i, T) = 1 / c_T[/math].
	    	Ändlig: [math]beta_(i, T) = a_(i, N+1), hat beta_(i, T) = beta_(i, T) / (c_T c_(T+1))[/math]

	    	[b]Steg ([math]t = T - 1, ..., 1[/math])[/b].
	    	[math]hat beta_(i, t) = 1 / c_t sum_(j=1)^N a_(i, j) b_j (x_(t+1)) hat beta_(j, t + 1)[/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Hur kan man beräkna sannolikheten för ett viss tillstånd vid en tidpunkt, givet [i]alla[/i] observationer.
	    </Question>
	    <Answer>
	    	Det kan man göra med att kombinera forward och backward algoritmen. Anledningen att man inte endast kan använda forward algoritmen är att det är möjligt att via senare observationer att sannolikheten för ett visst tillstånd faktiskt vara mycket mindre än vad vi trodde när vi bara tog hänsyn till nuvarande observationer.

	    	[math]P[S_t = j | x_1, ..., x_T, lambda] = gamma_(j, t) = (alpha_(k, t) beta_(j, t)) / (c_1 cdots c_T).[/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad beräknar Viterbi algortimen?
	    </Question>
	    <Answer>
	    	Den mest sannolika sekvensen av tillstånd, givet observationerna (enligt MAP regeln), alltså:
	    	[b]Oändlig[/b]
	    	[math]hat ul i = hat (i_1, ..., i_T) = argmax_(i_1, ..., i_T) P[S_1 = i_1, ..., S_T = i_T | x_1, ..., x_T, lambda][/math]

	    	[b]Ändlig[/b]
	    	[math]hat ul i = hat (i_1, ..., i_T) = argmax_(i_1, ..., i_T) P[S_1 = i_1, ..., S_T = i_T, S_(T+1) = text{exit} | x_1, ..., x_T, lambda][/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Hur fungerar Viterbi algoritmen?
	    </Question>
	    <Answer>
	    	Först, låt oss introducera några variabler:
	    	[b]Viterbi Probability Vector[/b]
	    	[math]chi_(j, t) = max_(i-1, ..., i_(t-1)) P[S_1 = i_1, ..., S_(t-1) = i_(t-1), S_t = j, x_1, ..., x_t | lambda][/math]. [math]chi_(j, t)[/math] indikerar sannolikheten för den bästa partiella kandidat sekvensen  som slutar i tillstånd [math]S_t = j[/math] vid tidpunkt [math]t[/math].

	    	[b]Viterbi Backpointer Matrix[/b]
	    	[math]ς_(j, t) = argmax_i chi_(i, t-1) a_(i, j)[/math]. [math]ς_(j, t)[/math] indikerar för varje möjligt tillstånd [math]S_t = j[/math] det tidigare tillståndet [math]S_(t-1)[/math] av den bästa partiella kandidat sekvensen som slutar i tillstånd [math]S_t = j[/math] vid tidpunkt [math]t[/math].

	    	[b]Initialisering ([math]t = 1[/math])[/b]
	    	[math]chi_(j, 1) = P[S_1 = j, x_1] = q_j b_j(x_1)[/math]

	    	[b]Framåtsteg ([math]t = 2, ..., T)[/math][/b]
	    	[math]chi_(j, t) = b_j(x_t) max_i chi_(i, t-1) a_(i ,j)[/math]
	    	[math]ς_(j, t) = argmax_i chi_(i, t - 1) a_(i, j)[/math]

	    	[b]Terminering (för ändliga)[/b]
	    	[math]chi_(j, T) = b_j(x_T) a_(j, N+1) max_i chi_(i, T-1) a_(i, j)[/math]
	    	[math]ς_(j, T) = argmax_i chi_(i, T - 1) a_(i, j)[/math]

	    	[b]Backtracking ([math]t = T[/math])[/b]
	    	[math]hat i_T = argmax_i chi_(i, T)[/math]
	    	[math]hat i_t = ς_(hat i_(t+1), t+1)[/math] för [math]t = T-1, T-2, ..., 1[/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Vad gör Baum-Welch algoritmen?
	    </Question>
	    <Answer>
	    	Givet en mängd av träningsdata, [math]ul x[/math], så algoritmen beräknar den modell [math]lambda_(ML) = {{ q, A }, B}[/math] med högst sannolikhet att generera träningsdatan: [math]lambda_(ML) = argmax_(lambda) P[ul x | lambda][/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="HMM">
	    <Question>
	    	Hur fungerar Baum-Welch algoritmen?
	    </Question>
	    <Answer>
	    	[b]Notation[/b]
	    	Låt [math]ul x = ((x_1^1, ..., X_(T_1)^1), (x_1^2, x_(T_2)^2), ..., (x_1^R, ..., x_(T_R)^R))[/math] där varje element i [math]ul x[/math] är en sekvens av observationer.

	    	[b]Initialisering[/b]
	    	Hitta på något sätt [math]lambda^0 = {{ q, A }, B}[/math].

	    	[b]Iterera för [math]n = 1, 2, ...[/math][/b]
	    	Hitta en förbättrad modell så att [math]log P[ul x | lambda^n] > log P[ul x | lambda^(n-1)][/math].

	    	[i]Uppdatera [math]q[/math][/i]
	    	[math]q_j^(text{new}) = 1 / R sum_(r=1)^R P[S_1^r = j | x_1^r, ..., x_(T_r)^r, lambda][/math]
	    	För att beräkna behöver vi:
	    	[math]gamma_(j, 1)^r = hat alpha_(j, 1)^r hat beta alpha_(j, 1)^r c_1^r[/math]
			[math]bar gamma_j = sum_(r=1)^R gamma_(j, 1)^r[/math] för alla [math]j[/math]
			[math]q_j^(text{new}) = bar gamma_j / (sum_(k=1)^N bar gamma_k)[/math]

			[i]Uppdatera [math]A[/math][/i]
			[math]xi_(i, j, t)^r = hat alpha_(i, t)^r a_(i, j) b_j(x_(t+1)^r) hat beta_(j, t+1)^r[/math]
			[math]bar xi_(i, j) = sum_(r=1)^R sum_(t=1)^(T_r-1) xi_(i, j, t)^r[/math]
			[math]a_(i, j)^(text{new}) = bar xi_(i, j) / (sum_k bar xi_(i, k))[/math]

			[i]Uppdatera [math]B[/math][/i]
			För fallet där observationerna är diskreta:
			[math]bar b_(j, m) = sum_(r=1)^R sum_(t=1)^(T_r) gamma_(j, t)^r delta_(z_t^r, m)[/math]
			[math]delta_(z_t^r, m) = { (1, text{när } z_t^r = m), (0, text{annars}) :}[/math]
			[math]b_(j, m)^(text{new}) = bar beta_(j, m) / (sum_(k=1)^M bar beta_(j, k))[/math]

	    	[b]Terminering[/b]
	    	När modellen har konvergerat, [math]log P[ul x | lambda^n] - log P[ul x | lambda^(n-1)] lt Delta_{text{min}}[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="EM">
	    <Question>
	    	Vad används Expectation Maximization till?
	    </Question>
	    <Answer>
	    	Den används för att estimera parametrarna för en probabilistisk modell, som beror på en gömd variabel [math]S[/math] som vi inte kan observera, men vi kan observera [math]X[/math] som beror på [math]S[/math].
	    </Answer>	
	</TestQuestion>
	<TestQuestion Category="EM">
	    <Question>
	    	Hur fungerar Expectation Maximization algoritmen?
	    </Question>
	    <Answer>
	    	Först, låt oss introducera: [math]Q(lambda', lambda) = E_(ul S)[log P[ul S, ul x | lambda'] | ul x, lambda][/math].

	    	[b]Initialisering[/b]
	    	Hitta på något sätt initialvärden för parametrarna, [math]lambda^0[/math].

	    	[b]Iterera för [math]n = 1, 2, ...[/math][/b]
	    	Hitta en nya parameterar [math]lambda^n[/math] som är bättre än tidigare, [math]P[x_1, ..., x_T |lambda^n] > P[x_1, ..., x_T |lambda^(n-1)][/math]. Dessa ska väljas enligt: [math]lambda^n = argmax_(lambda') Q(lambda', lambda^(n-1))[/math].

	    	[b]Terminering[/b]
	    	Avsluta efter ett fixt antal iterationer eller när: [math]Q(lambda^n, lambda^(n-1)) - Q(lambda^(n-1), lambda^(n-1)) lt Delta_min[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="EM">
	    <Question>
	    	Vad är en Gaussian Mixture Model (GMM) för något?
	    </Question>
	    <Answer>
	    	Det är en viktad summa av [math]M[/math] olika normalfördelade variabler. En GMM kan approximera en godtycklig fördelning med godtycklig precision genom att öka antalet komponenter.

	    	Om [math]X[/math] är en skalär, så är PDF: [math]f_X(x) = sum_(m=1)^M w_m 1 / (sigma_m sqrt(2 pi)) e^(-(x-mu_m)^2 / (2 sigma_m^2))[/math].
	    	Om [math]X[/math] är en vektor så är PDF: [math]f_X(x) = sum_(m=1)^M w_m 1 / ((2 pi)^(K/2) sqrt(det C_m)) e^(-1/2 (x-mu_m)^T C_m^(-1) (x-mu_m))[/math].

	    	För att ovan ska bli en fördelning, så gäller det att: [math]sum_(m=1)^M w_m = 1[/math].
	    </Answer>
	</TestQuestion>
<!-- 	<TestQuestion Category="EM">
	    <Question>
	    	Vad är väntevärdet och variansen för en GMM
	    </Question>
	    <Answer>

	    </Answer>
	</TestQuestion> -->
	<TestQuestion Category="EM">
	    <Question>
	    	Hur kan vi estimera parametrarna i en GMM?
	    </Question>
	    <Answer>
	    	Med hjälp av Expectation Maximization. Låt oss introducera variabeln [math]U_t[/math] som varje observation [math]X_t[/math] beror på, som vi ej kan observera. Distributionen av variabeln [math]X_t[/math] är då resultatet av att:
	    	[ul]
	    		[li]Först välja slumpvariabeln [math]U_t[/math] från [math]U_t = m in { 1, ..., M }[/math] med sannolikhet [math]P[U_t = m] = w_m[/math], oberoende av [math]t[/math].[/li]
	    		[li]Sedan, givet att [math]U_t = m[/math], ett värde [math]X_t[/math] väljs från den  [math]m[/math]:te normalfördeningen.[/li]
	    	[/ul]

	    	[b]Notation[/b]
	    	Låt oss definera [math]f_(X_t|U_t)(x | m) = g(x, mu_m, C_m) = text{PDF för flervariabels normalfördelning}[/math].
	    	Då ges [math]gamma_(m, t) = P[U_t = m | x_t, theta] = (w_m g(x_t, u_m, C_m)) / (sum_(k=1)^M w_k g(x_t, mu_k, C_k))[/math].

	    	[b]Uppdatera vikterna[/b]
	    	[math]w_m^(text{new}) = (sum_(t=1)^T gamma_(m, t)) / (sum_(k=1)^M sum_(t=1)^ T gamma_(k, t))[/math] för [math]m = 1, ..., M[/math].

	    	[b]Uppdatera parametrarna[/b]
	    	[math]mu_m^(text{new})=(sum_(t=1)^T gamma_(m, t) x_t) / (sum_(t=1)^T gamma_(m, t))[/math]
	    	[math]C_m^(text{new}) = (sum_(t=1)^T gamma_(m, t) (x_t - mu_m^(text{new})) (x_t - mu_m^(text{new}))^T) / (sum_(t=1)^T gamma_(m, t))[/math]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="EM">
	    <Question>
	    	Vad är en Lagrangemultiplikator?
	    </Question>
	    <Answer>
	    	Det är en metod för att optimera en funktion [math]f(x, y)[/math] med begräsningar av formen [math]g(x, y) = c[/math].

	    	Den fungerar så att vi introducerar en ny varaibel [math]lambda[/math], lagrangemultiplikatorn. Som definieras av [math]L(x, y, lambda) = f(x, y) - lambda * (g(x, y) - c)[/math].

	    	För att lösa optimeringsproblemet så hittar vi där gradienten av [math]L[/math] är noll, dvs: [math]nabla_(x, y, lambda) L(x, y, lambda) = 0[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad är den huvudsakliga skillnaden mellan Bayesian Learning och Maximum Likelihood?
	    </Question>
	    <Answer>
	    	I Bayesian Learning så antar vi att också parametrarna [math]w[/math] är ett utfall ifrån en slumpvariabel [math]W[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Hur fungerar Bayesian Learning?
	    </Question>
	    <Answer>
	    	[ol]
	    		[li]Samla in träningsdata [math]D=ul x = (x_1, ..., x_T)[/math] som består av feature vektorer [math]x_t[/math]. Varje feature vektor anses som ett utfall av slumpvariabeln [math]X_t[/math].[/li]
	    		[li]Formulera en modell för conditional feature-vector density, [math]f_(X|W)(x | w)[/math] där [math]w[/math] är parametrarna som anses vara ett utfall från slumpvariabeln [math]W[/math]. Vi antar också att alla feature vektorer i sekvensen är oberoende, detta ger att: [math]f_(ul X | W) (ul x | w) = prod_(t=1)^T f_(X | W)(x_t | w)[/math].[/li]
	    		[li]Välj en form för prior density function för parameter vektorn: [math]f_W(w; a^((0)))[/math] där [math]a^((0))[/math] är hyper-parametrarna.[/li]
	    		[li]Beräkna Likelihood funktionen: [math]f_(ul X, W)(x, w) = f_(ul X | W)(ul x | w) f_W (w; a^((0)))[/math][/li]
	    		[li]Identifiera posterior density function för [math]W[/math]: [math]f_(W|ul X)(w | ul x; a^((T))) = c(ul x) f_(ul X | W)(ul x | w) f_W(w ; a^((0)))[/math] där [math]c(ul x) = P[ul X = ul x][/math] (normaliserings konstant).[/li]
	    		[li]Beräkna predictive density function: [math]f_(X | ul X)(x | ul x; a^((T))) = int_(text{alla } w) f_(X|W)(x | w) f_(W  | ul X)(w | ul x; a^((T))) dw[/math]. Denna funktion används sedan som discriminant funktion.[/li]
	    	[/ol]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad är för- och nackdelar med Bayesian Learning?
	    </Question>
	    <Answer>
	    	[b]Fördelar[/b]
	    	[ul]
	    		[li]Klassificeraren använder inte endast en mängd av modell parameterar, utan är medelvärdet av alla.[/li]
	    		[li]Den tränade modellen innehåller en indikator på sin egen pålitlighet.[/li]
	    		[li]Modellen kan enkelt tränas online, dvs. anpassa sig efter ny data.[/li]
	    		[li]Det är möjligt att låta träningsalgoritmen själv bestämma modellens komplexitet.[/li]
	    	[/ul]
	    	[b]Nackdelar[/b]
	    	[ul]
	    		[li]Beräkningarna blir oftast mer komplicerade.[/li]
	    		[li]Det är ofta inte möjligt att hitta en exakt analytiskt lösning[/li]
	    		[li]Vi måste välja en initial distribution för modell parametrarna. Det existerar ingen formell metod för att göra det som fungerar i alla situationer.[/li]
	    	[/ul]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad innebär det att en parameter distribution är konjugat?
	    </Question>
	    <Answer>
	    	Det innebär att vi väljer en prior distribution så att en posterior parameter distribution efter träning har samma matematiska form, endast med modifierade hyper-parameters.
	    	Exempel är:
	    	[ul]
	    		[li]Beta[/li]
	    		[li]Dirichlet[/li]
	    		[li]Gamma[/li]
	    	[/ul]
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad innebär att en prior är Subjective Informative?
	    </Question>
	    <Answer>
	    	Att [math]f_W(w)[/math] kan tolkas som ett mått på den subjektiva tron att [math]W[/math] har ett värde nära [math]w[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad innebär en improper prior?
	    </Question>
	    <Answer>
	    	Det innebär att [math]f_X(x) \rightarrow text{konstant}[/math] på intervalet [math][-\infty, +infty][/math], men kan ej normaliseras. En improper prior kan approximeras av en proper density function, t.ex. normalfördelning med 0 i väntevärdet och standardavvikelse som går emot oändligheten.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad innebär det att en prior är Subjective Non-Informative?
	    </Question>
	    <Answer>
	    	<!-- Man bör välja en prior density funktion som har så lite som möjligt påverkan på posterior density för parametrarna. -->
	    	Hitta svar på detta!
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	När är valet av prior density funktion viktigt, och när är det mindre viktigt?
	    </Question>
	    <Answer>
	    	När det finns ett fåtal observationer är det viktigt, och när det finns många så är valet mindre viktigt. Vad "fåtal" och "många" innebär vet jag inte.
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad är Fisher Information?
	    </Question>
	    <Answer>
	    	Om en slumpvariabel [math]X[/math] har en density funktion [math]f_(X|W)(x | w)[/math], specificerat av någon parameter vektor [math]w[/math]. Då är Fisher Information, [math]I(w)[/math] en matris med elementen: [math]I_(i, j)(w)=E_X[(del ln f_(X|W)(X |w)) / (del w_i) (del ln f_(X|W)(X |w)) / (del w_j) | w][/math]. Den går också att beräkna som: [math]I_(i, j)(w) = -E_X[(del^2 ln f_(X|W)(X | w)) / (del w_i del w_j) | w][/math].

	    	Om [math]w[/math] endast är en skalär, reduceras det till: [math]I(w) = E_X[((del ln f_(X|W)(X | w)) / (del w))^2 | w][/math].

	    	Det gäller även att för en unbiased estimator [math]hat w[/math]: [math]V[hat w] ge 1 / (I(hat w))[/math].

	    	Om F.I. är stort, så kommer experiment i medel att ge mer precis information om parametern, och när F.I: är mindre så kommer vi en mindre precis uppskattning.

	    	För varje mängd av utfall av [math]X[/math] som påverkas av ett sanna värdet är [math]W = w[/math] så gäller det att [math]W in (hat w pm (Delta w) / 2)[/math] med en fixt sannolikhet för varje [math]w[/math], där [math]Delta w ~~ sqrt(V[hat w])[/math].
	    </Answer>
	</TestQuestion>
	<TestQuestion Category="BayesianLearning">
	    <Question>
	    	Vad är Jeffreys prior för något?
	    </Question>
	    <Answer>
	    	Det är en typ av objektiv non-informative prior. Om en slumpvariabel [math]X[/math] har en density funktion som specificeras som ett utfall av en slump parameter vektor [math]W=w[/math], då gäller det att [math]f_W(w) prop sqrt(det(I(w)))[/math]. Det gäller även om [math]U=g(W)[/math] för någon funktion [math]g[/math].
	    </Answer>
	</TestQuestion>
</Test>
